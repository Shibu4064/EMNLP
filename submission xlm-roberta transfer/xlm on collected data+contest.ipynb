{"cells":[{"cell_type":"markdown","metadata":{"id":"9dBC0DJLIEKX"},"source":["<h1>Extracting base finetuned(with BD-SHS dataset) from the finetuned model(Did not run the code yet)<h1>"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12630,"status":"ok","timestamp":1692336782244,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"F3xXUMDVfRI7","outputId":"d670a06a-657f-4728-d835-50da479b0785"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install --quiet transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"KJs3tsZEdtJP","executionInfo":{"status":"ok","timestamp":1692336788609,"user_tz":-360,"elapsed":6369,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["import time\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from tqdm.notebook import tqdm\n","from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AutoModelForMaskedLM, AutoTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"OR9A--a-jmNA","executionInfo":{"status":"ok","timestamp":1692336788610,"user_tz":-360,"elapsed":6,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["#defining some hyperparameters\n","max_number_input_tokens=256\n","batch_size_training = 16\n","first_dropout_rate = 0.3\n","hidden_output = 768\n","bert_model_name = \"xlm-roberta-base\"\n","adam_opt_lr = 3e-5\n","scheduler_step = 1\n","scheduler_gamma = 0.8\n","epochs = 10\n","classes = 2\n","need_split_dataset=False"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4455,"status":"ok","timestamp":1692336953882,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"rXg4692CZ5cb","outputId":"1ac8c62c-927f-4de3-844c-375f3162e851"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DirPath = ('/content/drive/My Drive/Test/')\n","Finetuned_model_path = DirPath+bert_model_name+\"_CustomBertBengaliFullDataset6epoch885044valacc.pth\"\n","CollectedDatasetFileName = \"Final_data.csv\"\n","CollectedDatasetPath = DirPath+\"EMNLP/\"+CollectedDatasetFileName\n","SplittedTrainFileName = \"train.csv\"\n","SplittedValFileName = \"dev.csv\"\n","SplittedTrainDataPath = DirPath+\"EMNLP/\"+SplittedTrainFileName\n","SplittedValDataPath = DirPath+\"EMNLP/\"+SplittedValFileName\n","\n","SplittedTrainFileName = \"final_data_train.csv\"\n","SplittedValFileName = \"final_data_val.csv\"\n","SplittedTrainCDataPath = DirPath+\"collected dataset after split/\"+SplittedTrainFileName\n","SplittedValCDataPath = DirPath+\"collected dataset after split/\"+SplittedValFileName"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"AEBojsPoiA_b","executionInfo":{"status":"ok","timestamp":1692336953883,"user_tz":-360,"elapsed":4,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["def interchange(df_train,pos,label):\n","  #setting the first sample to be with label '0'\n","  zero_index = df_train[df_train['label'] == label].index[0]\n","  first_index=pos\n","  # interchange the samples\n","  df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","  return df_train"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"EmUATD-1YehW","executionInfo":{"status":"ok","timestamp":1692336953883,"user_tz":-360,"elapsed":3,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["def balanceclasses(df_train):\n","  class_counts = df_train['label'].value_counts()\n","  min_count = class_counts.max()\n","\n","  # Create new DataFrames for each class with fewer samples\n","  new_dfs = []\n","  for label, count in class_counts.items():\n","    if count == min_count:\n","        continue\n","    df_label = df_train[df_train['label'] == label]\n","    num_copies = min_count // count\n","    new_df_label = pd.concat([df_label] * num_copies, ignore_index=True)\n","    new_df_label = new_df_label.head(min_count-count)\n","    #print(new_df_label.head(10))\n","    new_dfs.append(new_df_label)\n","\n","  # Concatenate the new DataFrames with the original DataFrame\n","  df_balanced = pd.concat([df_train] + new_dfs, ignore_index=True).sample(frac=1).reset_index(drop=True)\n","  return df_balanced"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2502,"status":"ok","timestamp":1692336963161,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"mXlHCNIrflUj","outputId":"90cfccbd-1476-4aac-f7b9-f2dd481473cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["(9225, 2)\n","(1330, 2)\n","Train label counts:\n"," 1.0    4888\n","2.0    2678\n","0.0    1659\n","Name: label, dtype: int64\n","Validation label counts:\n"," 0    717\n","1    417\n","2    196\n","Name: label, dtype: int64\n","\n"," after making copies:\n","1.0    4888\n","0.0    4888\n","2.0    4888\n","Name: label, dtype: int64\n","                                                    text  label\n","0                   সোনার ছেলেরা মারা মারী করতে ভালো পায়    0.0\n","1      কুকুর লিগের কুকুর গুলো সত্য বললে আর শুনলে বি এ...    1.0\n","2      যে নারী বিয়ে করতে চায় না বুঝতে হবে সে সেক্স বি...    2.0\n","3              আলীর  গুষ্ঠীরাই বেশি  খাচ্চর শালা মাগিবাজ    2.0\n","4      পতিতাদের চরিত্র রাজনৈতিক নেতাদের থেকে হাজার গু...    1.0\n","...                                                  ...    ...\n","14659  একদিন এক দিন মজিব এর বঙ্গবন্ধু খেতাব ও কেরে নি...    2.0\n","14660  ঘটনা যদি সত্যি হয়ে থাকে তাহলে তাকে প্রকাশ্যে ফ...    2.0\n","14661  ভোট চোর মাফিয়া হাসিনারে মারা জন‍্য বিএনপি লাগব...    2.0\n","14662  পৃথিবীটা আল্লাহ সৃষ্টি করেছেন।তাই আল্লাহর হুকু...    0.0\n","14663  একটা কথায় আছে না? বৃক্ষ তোমার নাম কি,, ফলে তা...    1.0\n","\n","[14664 rows x 2 columns]\n","                                                   text  label\n","0     পাডা পুতার মাঝখানে পরে সাধারণ ২ মানুষের জিবন শ...      0\n","1     করোনার চাপে অনেক কিছু বন্ধ ও অনেক বিধি নিষেধ ক...      0\n","2     সঠিক তদন্ত করতে হবে। বিচারের আওতায় আনতে হবে য...      0\n","3     যে লোকটা মারা গেছে তার কি হবে তার দায়ভার কে ন...      0\n","4     নিউ মার্কেট এবং গুলিস্থান মার্কেটের ব্যবসায়ীর...      1\n","...                                                 ...    ...\n","1325  নাটক টা সুন্দর ভাবে সাজিয়েছে আরো কত কিছু দেখত...      1\n","1326  নোংরা দেশ আর নোংরা জাতি হচ্ছে ভারত এঁরা কি বুঝ...      1\n","1327                        জে ছেলে মারা গেছে ওর কি হবে      0\n","1328                           এরাই নৈরাজ্য সৃষ্টি করছে      1\n","1329  ব্রাহ্মণবাড়িয়ার হত্যা হলে, হেডলাইনে ব্রাহ্মণবা...      0\n","\n","[1330 rows x 2 columns]\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","#splitting the dataset and saving\n","if need_split_dataset==True:\n","  #dataset loading\n","  df = pd.read_csv(CollectedDatasetPath)[ ['Text','label'] ]\n","  print(f'df label counts\\n',df['label'].value_counts())\n","  # check if there is any NaN value in the dataframe\n","  print(f'null values: {df.isna().sum()}')\n","\n","  #null indices\n","  null_index = df.index[df.isna().any(axis=1)]\n","  print(f'null indices: {null_index}')\n","\n","  #dropping null values\n","  df = df.dropna()\n","\n","  df_train, df_val = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n","  df_train.to_csv(SplittedTrainDataPath)\n","  df_val.to_csv(SplittedValDataPath)\n","else:\n","  df_train = pd.read_csv(SplittedTrainCDataPath)[ ['Text','label'] ]\n","  df_val = pd.read_csv(SplittedValCDataPath)[ ['Text','label'] ]\n","\n","df_train = pd.concat([df_train, df_val], ignore_index=True)\n","# df_val = df_train\n","df_train = df_train.rename(columns={'Text': 'text'})\n","\n","df_train2 = pd.read_csv(SplittedTrainDataPath)[ ['text','label'] ]\n","df_val = pd.read_csv(SplittedValDataPath)[ ['text','label'] ]\n","\n","df_train = pd.concat([df_train, df_train2], ignore_index=True)\n","\n","# count the number of each unique label in train and validation dataframes\n","train_label_counts = df_train['label'].value_counts()\n","val_label_counts = df_val['label'].value_counts()\n","\n","#setting the first sample to be with label '0'\n","zero_index = df_train[df_train['label'] == 0].index[0]\n","first_index=0\n","# interchange the samples\n","df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","print(df_train.shape)\n","print(df_val.shape)\n","print('Train label counts:\\n', train_label_counts)\n","print('Validation label counts:\\n', val_label_counts)\n","\n","print(\"\\n after making copies:\")\n","#balance all classes making copies\n","df_train = balanceclasses(df_train)\n","print(df_train['label'].value_counts())\n","\n","#setting the first sample to be with label '0'\n","zero_index = df_train[df_train['label'] == 0].index[0]\n","first_index=0\n","# interchange the samples\n","df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","#setting the first sample to be with label '0'\n","zero_index = df_train[df_train['label'] == 1].index[0]\n","first_index=1\n","# interchange the samples\n","df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","print(df_train)\n","print(df_val)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Z-k2B2ThB8SY","executionInfo":{"status":"ok","timestamp":1692336982200,"user_tz":-360,"elapsed":741,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["# #df loading\n","# df_train = pd.read_csv('train.csv')[['sentence','hate speech']]\n","# df_val = pd.read_csv('val.csv')[['sentence','hate speech']]\n","# df_test = pd.read_csv('test.csv')[['sentence','hate speech']]\n","\n","# #concatenating all the data\n","# df_train = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","\n","# print(df_train.shape)\n","# print(df_val.shape)\n","# print(df_test.shape)\n","# print(df_train)\n","# print(df_train.describe())"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"eYqqzO0Qfa3t","executionInfo":{"status":"ok","timestamp":1692336982818,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['text'] , value['label']"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"vA643oD4ms6K","executionInfo":{"status":"ok","timestamp":1692336982818,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_val)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"EteIRQlRp2vw","executionInfo":{"status":"ok","timestamp":1692336989398,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class HateSpeechBert(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(HateSpeechBert, self).__init__()\n","\n","        self.bert = bert\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(first_dropout_rate)\n","\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(hidden_output*2, hidden_output)\n","\n","        #dense layer 2\n","        self.fc2 = nn.Linear(hidden_output, 128)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc3 = nn.Linear(128, 2)\n","\n","        #softmax\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        # pass the inputs to the model\n","        out = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","\n","        mean, _ = torch.max(out[0], 1)\n","        x= torch.cat((mean,out[1]), dim=1)\n","\n","        x = self.dropout(x)\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","\n","        # output layer\n","        x = self.fc3(x)\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"jVea3LVdlfpt","executionInfo":{"status":"ok","timestamp":1692336990236,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class BERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengali, self).__init__()\n","        #self.bert = BertForMaskedLM.from_pretrained(\"sagorsarker/bangla-bert-base\")\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(0.2)\n","        self.out = nn.Linear(hidden_output, 2)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        bo = self.bert_drop(output[1])\n","\n","        output = self.out(bo)\n","        return output"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"w7VapuTb3lzo","executionInfo":{"status":"ok","timestamp":1692336991517,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class BERTBengaliPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliPooler, self).__init__()\n","        self.bert = bert\n","        #self.bert.pooler.dense = nn.Linear(bert.config.hidden_size, bert.config.hidden_size)\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.out = nn.Linear(bert.config.hidden_size, classes)\n","        #softmax\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        pooled_output = outputs.pooler_output\n","        bo = self.bert_drop(pooled_output)\n","\n","        output = self.out(bo)\n","        output = self.softmax(bo)\n","        return output"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"qWAAuIfnn-nI","executionInfo":{"status":"ok","timestamp":1692336992373,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 3, classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        o1 = outputs.hidden_states[-1]\n","        o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        pooled_output = o2\n","        cat = torch.cat((apool, mpool, pooled_output), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"cNtrCwvch5nI","executionInfo":{"status":"ok","timestamp":1692336993210,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class BERTBengaliTwo(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliTwo, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 2, classes)\n","        torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        out = torch.cat((outputs.hidden_states[-1], outputs.hidden_states[-2]), dim=-1)\n","        out = self.drop_out(out)\n","        out = out[:,0,:]\n","        logits = self.l0(out)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"EAFjVWK8SOkP","executionInfo":{"status":"ok","timestamp":1692336993831,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class BERTBengaliLastTwoPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPooler, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 3, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        #out = out[:,0,:]\n","        logits = self.l0(out)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":485,"referenced_widgets":["85922646f6db472588d1663a9f2f2b01","b6650677071f44b3b9ffaf4bc1f02751","3255649900b54afcbe7f1bd644b8b9ec","649c6851315542988880d6db7ac4222d","5aa4eabea5034523b23fd9a269e69624","c36c4a1015f54be89cad12277d71ddcd","520fd9e2b9344d3eb21e8bd6f346b764","16bdca32887b4be7a5f947b18a0cd997","35aa0b29a8da4bb5a5229733a098158e","af07f82d928f4ce9ad3f2a2f16026e05","535265ad812f4382b38c61972d43c56b","f7f818a2b4794253b643e3cb3388a719","0f2d5becc6d7484f841c62a172113bf2","1b9e8f19227247659601d53638f7d563","44927cb92c0a4163bd310c91812a9b3e","84ddc07964b049fa8003fa7e90eb2a25","a40f982b19124c14b3599836ddfd21a9","10a8f9cb016849ed9757ed0f3418aa58","c860a22e04af488dbfcd51363f35ae1c","862b8f27216648fd9e83a9ead6826dc5","66a9988006504e37a8c8aca355332cdb","6555f9a9066e4fcaace9cfbca3c074ab"]},"executionInfo":{"elapsed":12791,"status":"error","timestamp":1692337007288,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"X27bxqPFuaro","outputId":"a38a606e-f4ba-4cc8-b611-cbed381140c5"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85922646f6db472588d1663a9f2f2b01"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7f818a2b4794253b643e3cb3388a719"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-3c332b464ed9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   1826\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'xlm-roberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'xlm-roberta-base' is the correct path to a directory containing all relevant files for a BertTokenizer tokenizer."]}],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"2YL9gaGTuocH","colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"status":"error","timestamp":1692337027962,"user_tz":-360,"elapsed":6060,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}},"outputId":"b347ab0a-987d-4e5a-8133-8cd623c483a1"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-c1d19d681b50>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#creating the structure to contain finetuned bert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstruct_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomBERTBengali\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstruct_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#loading the finetuned model which have leaned necessary info from other domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m     def register_full_backward_pre_hook(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1142\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#creating the structure to contain finetuned bert\n","struct_model = CustomBERTBengali(bert)\n","struct_model.to(device)\n","\n","#loading the finetuned model which have leaned necessary info from other domain\n","struct_model.load_state_dict(torch.load(Finetuned_model_path))\n","\n","# Access the bert model\n","finetuned_bert_base = struct_model.bert"]},{"cell_type":"markdown","metadata":{"id":"nhygxywpTztv"},"source":["<h1>Creating a model with transferred learned knowledge capable of being finetuned with collected data and training it\n","##Note: test data and val data are same here.<h1>"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"7Q8iG3vluRLA","executionInfo":{"status":"ok","timestamp":1692337032536,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["###Hyperparameter for the new model\n","#defining some hyperparameters\n","max_number_input_tokens=256\n","batch_size_training = 8\n","first_dropout_rate = 0.0\n","hidden_output = 768\n","bert_model_name = \"xlm-roberta-base\"\n","adam_opt_lr = 3e-6\n","scheduler_step = 1\n","scheduler_gamma = 0.98\n","epochs = 100\n","classes = 3\n","#need_split_dataset=False"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"MtIzKtM_xqHt","executionInfo":{"status":"ok","timestamp":1692337033140,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['text'] , value['label']"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"aK9mkfgDxrVZ","executionInfo":{"status":"ok","timestamp":1692337037484,"user_tz":-360,"elapsed":602,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=False)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_val)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"PSjqu8UTubQA","executionInfo":{"status":"ok","timestamp":1692337040316,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["# #model for finetuning collected data\n","# class BERTBengaliLastTwoPooler(nn.Module):\n","#     def __init__(self, bert):\n","#         super(BERTBengaliLastTwoPooler, self).__init__()\n","#         self.bert = bert\n","#         self.drop_out = nn.Dropout(first_dropout_rate)\n","#         self.l0 =  nn.Linear(hidden_output * 3, classes)\n","#         #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","#         self.softmax = nn.Softmax(dim=1)\n","\n","#     def forward(self, input_ids, attention_mask, token_type_ids):\n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids\n","#         )\n","#         mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","#         out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","#         out = self.drop_out(out)\n","#         #out = out[:,0,:]\n","#         logits = self.l0(out)\n","#         logits = self.softmax(logits)\n","#         return logits"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"4_sM1dZ_Etmu","executionInfo":{"status":"ok","timestamp":1692337040962,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["# class CustomBERTBengali(nn.Module):\n","#     def __init__(self, bert):\n","#         super(CustomBERTBengali, self).__init__()\n","#         self.bert = bert\n","#         self.bert_drop = nn.Dropout(first_dropout_rate)\n","#         self.tanh = nn.Tanh()\n","#         self.out = nn.Linear(hidden_output * 3, classes)\n","#         self.softmax = nn.Softmax(dim=1)\n","\n","#     def forward(self, input_ids, attention_mask, token_type_ids):\n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids\n","#         )\n","#         o1 = outputs.hidden_states[-1]\n","#         o2 = outputs.pooler_output\n","#         apool = torch.mean(o1, 1)\n","#         mpool, _ = torch.max(o1, 1)\n","#         pooled_output = o2\n","#         cat = torch.cat((apool, mpool, pooled_output), 1)\n","#         bo = self.bert_drop(cat)\n","#         logits = self.out(bo)\n","#         #logits = self.softmax(logits)\n","#         return logits"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"BAH0JhR_gVdV","executionInfo":{"status":"ok","timestamp":1692337040963,"user_tz":-360,"elapsed":2,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class BERTBengaliLastTwoPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPooler, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 3, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        #out = out[:,0,:]\n","        logits = self.l0(out)\n","        # logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","source":["class BERTBengaliLastTwoPoolerPrev(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPoolerPrev, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 3, 2)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        #out = out[:,0,:]\n","        logits = self.l0(out)\n","        # logits = self.softmax(logits)\n","        return logits"],"metadata":{"id":"qwPxTZ-Olqlc","executionInfo":{"status":"ok","timestamp":1692337041858,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Cf54Z8g_qqzb","executionInfo":{"status":"ok","timestamp":1692337042642,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 2, 2)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids\n","        )\n","        # print(dict(outputs).keys())\n","        o1 = outputs.hidden_states[-1]\n","        # o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        # pooled_output = o2\n","        cat = torch.cat((apool, mpool), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"W2Yk-_A3Y6OH","executionInfo":{"status":"ok","timestamp":1692337043249,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["#model for finetuning collected data\n","class BERTBengaliLastTwoPoolerFreeze(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPoolerFreeze, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l2 = nn.Linear(hidden_output * 3, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l1 = nn.Linear(hidden_output * 2, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l0 = nn.Linear(hidden_output * 2, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((outputs.hidden_states[-2][:,0,:], mpool), dim=-1)#,outputs.pooler_output\n","        out = self.drop_out(out)\n","        out = self.l2(out)\n","        out = self.activation(out)\n","        out = self.l1(out)\n","        out = self.activation(out)\n","        logits = self.l0(out)\n","        #prob = self.softmax(logits)\n","        return logits\n"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"i4yc19NbQkJh","executionInfo":{"status":"ok","timestamp":1692337043854,"user_tz":-360,"elapsed":1,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["#model for finetuning collected data\n","class BERTBengaliLastTwoPoolerFreezePrev(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPoolerFreezePrev, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l1 = nn.Linear(hidden_output * 2, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l0 = nn.Linear(hidden_output * 2, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((outputs.hidden_states[-2][:,0,:], mpool,outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        out = self.l1(out)\n","        out = self.activation(out)\n","        logits = self.l0(out)\n","        #prob = self.softmax(logits)\n","        return logits\n"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["60e93f550d414e27ae56f6da3d030525","410b785a78e34c9a8c72d63284f7fc3d","bc542539167548f4a1723773cc440bc8","cf5923ecdb8b42d9a6ee84fbd7108a83","80d5b23bcbe0409cab4b9badae153a25","6a3052834b724bc18e1013b8d365d507","898a224d27f04b2491fd3ef1364fd134","1fee262b8c0b4efeb1548b2606d0eda4","1cc81e83864945418caf3c89e89edc79","30c91f083e0f426aa92ebff944b5319b","67a5d4e4293349a994a8d9fe0b00b2f6","0ac661561ac545c6a604585a41d5dfcf","1cd31355d60141599d835386fa9085e1","048ce0f0cf1b4d8d97e80dc5b79dc31e","58e97b093d09408aa6304ffeb6fdf5ea","b6a6d70840f6406baaf57d77c805c594","81d1213a26954c8496da9d1fb4382039","45dc909ccb6d4f4ea731b8832a91f532","e10517115efa4743ace023cf325a3f44","7c2161403ded49e9b8c717ec46212561","8aae7a65501a4ea1a0c1aacadb9aa3bf","006e3d13275c4c7eb2aa0caaf4ff044d"]},"executionInfo":{"elapsed":13196,"status":"ok","timestamp":1692337078385,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"9z9eXknsUPpV","outputId":"cc03307a-b063-45d1-ef89-3c2de17afd05"},"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of BertModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.intermediate.dense.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.7.intermediate.dense.bias', 'pooler.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'pooler.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.3.attention.self.value.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60e93f550d414e27ae56f6da3d030525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ac661561ac545c6a604585a41d5dfcf"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["name: bert.embeddings.word_embeddings.weight is trainable\n","name: bert.embeddings.position_embeddings.weight is trainable\n","name: bert.embeddings.token_type_embeddings.weight is trainable\n","name: bert.embeddings.LayerNorm.weight is trainable\n","name: bert.embeddings.LayerNorm.bias is trainable\n","name: bert.encoder.layer.0.attention.self.query.weight is trainable\n","name: bert.encoder.layer.0.attention.self.query.bias is trainable\n","name: bert.encoder.layer.0.attention.self.key.weight is trainable\n","name: bert.encoder.layer.0.attention.self.key.bias is trainable\n","name: bert.encoder.layer.0.attention.self.value.weight is trainable\n","name: bert.encoder.layer.0.attention.self.value.bias is trainable\n","name: bert.encoder.layer.0.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.0.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.0.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.0.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.0.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.0.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.0.output.dense.weight is trainable\n","name: bert.encoder.layer.0.output.dense.bias is trainable\n","name: bert.encoder.layer.0.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.0.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.1.attention.self.query.weight is trainable\n","name: bert.encoder.layer.1.attention.self.query.bias is trainable\n","name: bert.encoder.layer.1.attention.self.key.weight is trainable\n","name: bert.encoder.layer.1.attention.self.key.bias is trainable\n","name: bert.encoder.layer.1.attention.self.value.weight is trainable\n","name: bert.encoder.layer.1.attention.self.value.bias is trainable\n","name: bert.encoder.layer.1.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.1.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.1.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.1.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.1.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.1.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.1.output.dense.weight is trainable\n","name: bert.encoder.layer.1.output.dense.bias is trainable\n","name: bert.encoder.layer.1.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.1.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.2.attention.self.query.weight is trainable\n","name: bert.encoder.layer.2.attention.self.query.bias is trainable\n","name: bert.encoder.layer.2.attention.self.key.weight is trainable\n","name: bert.encoder.layer.2.attention.self.key.bias is trainable\n","name: bert.encoder.layer.2.attention.self.value.weight is trainable\n","name: bert.encoder.layer.2.attention.self.value.bias is trainable\n","name: bert.encoder.layer.2.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.2.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.2.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.2.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.2.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.2.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.2.output.dense.weight is trainable\n","name: bert.encoder.layer.2.output.dense.bias is trainable\n","name: bert.encoder.layer.2.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.2.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.3.attention.self.query.weight is trainable\n","name: bert.encoder.layer.3.attention.self.query.bias is trainable\n","name: bert.encoder.layer.3.attention.self.key.weight is trainable\n","name: bert.encoder.layer.3.attention.self.key.bias is trainable\n","name: bert.encoder.layer.3.attention.self.value.weight is trainable\n","name: bert.encoder.layer.3.attention.self.value.bias is trainable\n","name: bert.encoder.layer.3.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.3.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.3.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.3.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.3.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.3.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.3.output.dense.weight is trainable\n","name: bert.encoder.layer.3.output.dense.bias is trainable\n","name: bert.encoder.layer.3.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.3.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.4.attention.self.query.weight is trainable\n","name: bert.encoder.layer.4.attention.self.query.bias is trainable\n","name: bert.encoder.layer.4.attention.self.key.weight is trainable\n","name: bert.encoder.layer.4.attention.self.key.bias is trainable\n","name: bert.encoder.layer.4.attention.self.value.weight is trainable\n","name: bert.encoder.layer.4.attention.self.value.bias is trainable\n","name: bert.encoder.layer.4.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.4.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.4.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.4.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.4.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.4.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.4.output.dense.weight is trainable\n","name: bert.encoder.layer.4.output.dense.bias is trainable\n","name: bert.encoder.layer.4.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.4.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.5.attention.self.query.weight is trainable\n","name: bert.encoder.layer.5.attention.self.query.bias is trainable\n","name: bert.encoder.layer.5.attention.self.key.weight is trainable\n","name: bert.encoder.layer.5.attention.self.key.bias is trainable\n","name: bert.encoder.layer.5.attention.self.value.weight is trainable\n","name: bert.encoder.layer.5.attention.self.value.bias is trainable\n","name: bert.encoder.layer.5.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.5.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.5.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.5.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.5.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.5.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.5.output.dense.weight is trainable\n","name: bert.encoder.layer.5.output.dense.bias is trainable\n","name: bert.encoder.layer.5.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.5.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.6.attention.self.query.weight is trainable\n","name: bert.encoder.layer.6.attention.self.query.bias is trainable\n","name: bert.encoder.layer.6.attention.self.key.weight is trainable\n","name: bert.encoder.layer.6.attention.self.key.bias is trainable\n","name: bert.encoder.layer.6.attention.self.value.weight is trainable\n","name: bert.encoder.layer.6.attention.self.value.bias is trainable\n","name: bert.encoder.layer.6.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.6.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.6.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.6.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.6.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.6.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.6.output.dense.weight is trainable\n","name: bert.encoder.layer.6.output.dense.bias is trainable\n","name: bert.encoder.layer.6.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.6.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.7.attention.self.query.weight is trainable\n","name: bert.encoder.layer.7.attention.self.query.bias is trainable\n","name: bert.encoder.layer.7.attention.self.key.weight is trainable\n","name: bert.encoder.layer.7.attention.self.key.bias is trainable\n","name: bert.encoder.layer.7.attention.self.value.weight is trainable\n","name: bert.encoder.layer.7.attention.self.value.bias is trainable\n","name: bert.encoder.layer.7.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.7.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.7.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.7.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.7.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.7.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.7.output.dense.weight is trainable\n","name: bert.encoder.layer.7.output.dense.bias is trainable\n","name: bert.encoder.layer.7.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.7.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.8.attention.self.query.weight is trainable\n","name: bert.encoder.layer.8.attention.self.query.bias is trainable\n","name: bert.encoder.layer.8.attention.self.key.weight is trainable\n","name: bert.encoder.layer.8.attention.self.key.bias is trainable\n","name: bert.encoder.layer.8.attention.self.value.weight is trainable\n","name: bert.encoder.layer.8.attention.self.value.bias is trainable\n","name: bert.encoder.layer.8.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.8.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.8.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.8.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.8.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.8.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.8.output.dense.weight is trainable\n","name: bert.encoder.layer.8.output.dense.bias is trainable\n","name: bert.encoder.layer.8.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.8.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.9.attention.self.query.weight is trainable\n","name: bert.encoder.layer.9.attention.self.query.bias is trainable\n","name: bert.encoder.layer.9.attention.self.key.weight is trainable\n","name: bert.encoder.layer.9.attention.self.key.bias is trainable\n","name: bert.encoder.layer.9.attention.self.value.weight is trainable\n","name: bert.encoder.layer.9.attention.self.value.bias is trainable\n","name: bert.encoder.layer.9.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.9.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.9.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.9.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.9.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.9.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.9.output.dense.weight is trainable\n","name: bert.encoder.layer.9.output.dense.bias is trainable\n","name: bert.encoder.layer.9.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.9.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.10.attention.self.query.weight is trainable\n","name: bert.encoder.layer.10.attention.self.query.bias is trainable\n","name: bert.encoder.layer.10.attention.self.key.weight is trainable\n","name: bert.encoder.layer.10.attention.self.key.bias is trainable\n","name: bert.encoder.layer.10.attention.self.value.weight is trainable\n","name: bert.encoder.layer.10.attention.self.value.bias is trainable\n","name: bert.encoder.layer.10.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.10.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.10.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.10.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.10.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.10.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.10.output.dense.weight is trainable\n","name: bert.encoder.layer.10.output.dense.bias is trainable\n","name: bert.encoder.layer.10.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.10.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.11.attention.self.query.weight is trainable\n","name: bert.encoder.layer.11.attention.self.query.bias is trainable\n","name: bert.encoder.layer.11.attention.self.key.weight is trainable\n","name: bert.encoder.layer.11.attention.self.key.bias is trainable\n","name: bert.encoder.layer.11.attention.self.value.weight is trainable\n","name: bert.encoder.layer.11.attention.self.value.bias is trainable\n","name: bert.encoder.layer.11.attention.output.dense.weight is trainable\n","name: bert.encoder.layer.11.attention.output.dense.bias is trainable\n","name: bert.encoder.layer.11.attention.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.11.attention.output.LayerNorm.bias is trainable\n","name: bert.encoder.layer.11.intermediate.dense.weight is trainable\n","name: bert.encoder.layer.11.intermediate.dense.bias is trainable\n","name: bert.encoder.layer.11.output.dense.weight is trainable\n","name: bert.encoder.layer.11.output.dense.bias is trainable\n","name: bert.encoder.layer.11.output.LayerNorm.weight is trainable\n","name: bert.encoder.layer.11.output.LayerNorm.bias is trainable\n","name: bert.pooler.dense.weight is trainable\n","name: bert.pooler.dense.bias is trainable\n","name: l0.weight is trainable\n","name: l0.bias is trainable\n"]}],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","model = BERTBengaliLastTwoPooler(bert)\n","# model2Forlastlayers = BERTBengaliLastTwoPoolerPrev(bert)\n","\n","model.to(device)\n","# model2Forlastlayers.to(device)\n","# model2Forlastlayers.load_state_dict(torch.load(DirPath+'Models by Sami/'+bert_model_name+\"_lasttwopooler_fromHS_freezeencoder_f1.pth\"))\n","\n","# model.l0 = model2Forlastlayers.l0\n","# model.l2 = model2Forlastlayers.l1\n","# model.bert = model2Forlastlayers.bert\n","\n","# model.load_state_dict(torch.load(DirPath+'Models by Sami/'+bert_model_name+\"_lasttwopooler_fromcollected_f1.pth\"))\n","\n","for params in model.bert.embeddings.parameters():\n","  params.requires_grad = True\n","for params in model.bert.encoder.parameters():\n","  params.requires_grad = True\n","# for params in model.l2.parameters():\n","#   params.requires_grad = True\n","# for params in model.l1.parameters():\n","#   params.requires_grad = True\n","for params in model.bert.pooler.parameters():\n","  params.requires_grad = True\n","for params in model.l0.parameters():\n","  params.requires_grad = True\n","\n","for name, param in model.named_parameters():\n","  if param.requires_grad:\n","      print(f\"name: {name} is trainable\")\n","  else:\n","      print(f\"name: {name} is non-trainable\")"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3066,"status":"ok","timestamp":1692337087055,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"},"user_tz":-360},"id":"OmRT5yjsvdDK","outputId":"0e2c629f-a15d-46fa-e982-385fc414eaad"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[     0,  21145,  38732,   2801, 144840,  60420,   2730,    125,    378,\n","            294,  21290,    268,      2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n","tensor([[ 0.1945, -0.1637, -0.2933]], device='cuda:0',\n","       grad_fn=<AddmmBackward0>)\n"]}],"source":["#testing if the input of model works before starting training\n","s = \"আমি বাংলায় গান গাই। [SEP]\"\n","t = tokenizer.encode_plus(s, return_tensors=\"pt\").to(device)\n","print(t)\n","out = model(**t)\n","print(out)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"ZaJ8Ro_CxcPQ","executionInfo":{"status":"ok","timestamp":1692337087056,"user_tz":-360,"elapsed":7,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","\n","optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=adam_opt_lr)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"Q-YKSmNf1HrB","executionInfo":{"status":"ok","timestamp":1692337087056,"user_tz":-360,"elapsed":7,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, config):\n","    model.train()  # prep model for training\n","    train_loss = 0\n","    for batch in tqdm(dataloader):\n","        text, labels = batch\n","\n","        model.zero_grad()\n","\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        # token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        #labels = labels.to(device)\n","        labels = labels.to(device, dtype=torch.long)  # Convert labels to torch.long\n","\n","        # move things to model\n","        logs = model( input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss = criterion(logs, labels)\n","        #print(\"successfully calculated criterion in train!\")\n","        train_loss += loss.item() * input_ids.size(0)\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    return train_loss"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"WULrKYWT1jyv","executionInfo":{"status":"ok","timestamp":1692337087057,"user_tz":-360,"elapsed":7,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["def evaluate(model, dataloader, criterion, config):\n","    total = 0\n","    correct = 0\n","    valid_loss = 0.0\n","    label_0_TP = 0\n","    label_0_TN = 0\n","    label_0_FP = 0\n","    label_0_FN = 0\n","\n","    label_1_TP = 0\n","    label_1_TN = 0\n","    label_1_FP = 0\n","    label_1_FN = 0\n","\n","    label_2_TP = 0\n","    label_2_TN = 0\n","    label_2_FP = 0\n","    label_2_FN = 0\n","\n","    model.eval()  # prep model for evaluation\n","    for batch in dataloader:\n","        text, labels = batch\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        # token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        labels = labels.to(device, dtype=torch.long)\n","\n","        # move things to model\n","        output = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss_p = criterion(output, labels)\n","        # update running validation loss\n","        valid_loss += loss_p.item() * input_ids.size(0)\n","        # calculate accuracy\n","        proba = torch.exp(output)\n","        top_p, top_class = proba.topk(1, dim=1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        #print(f'predicted: {predicted} labels: {labels}')\n","        label_0_TP += ((predicted == 0) & (labels == 0)).sum().item()\n","        label_0_TN += ((predicted != 0) & (labels != 0)).sum().item()\n","        label_0_FP += ((predicted == 0) & (labels != 0)).sum().item()\n","        label_0_FN += ((predicted != 0) & (labels == 0)).sum().item()\n","\n","        label_1_TP += ((predicted == 1) & (labels == 1)).sum().item()\n","        label_1_TN += ((predicted != 1) & (labels != 1)).sum().item()\n","        label_1_FP += ((predicted == 1) & (labels != 1)).sum().item()\n","        label_1_FN += ((predicted != 1) & (labels == 1)).sum().item()\n","\n","        label_2_TP += ((predicted == 2) & (labels == 2)).sum().item()\n","        label_2_TN += ((predicted != 2) & (labels != 2)).sum().item()\n","        label_2_FP += ((predicted == 2) & (labels != 2)).sum().item()\n","        label_2_FN += ((predicted != 2) & (labels == 2)).sum().item()\n","\n","    return total, correct, valid_loss, label_0_TP, label_0_TN, label_0_FP, label_0_FN, label_1_TP, label_1_TN, label_1_FP, label_1_FN, label_2_TP, label_2_TN, label_2_FP, label_2_FN\n"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"Jfz6yeuc1pUZ","executionInfo":{"status":"ok","timestamp":1692337087057,"user_tz":-360,"elapsed":7,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}}},"outputs":[],"source":["\n","tokenizer_config = {\n","    \"max_length\": max_number_input_tokens,\n","    \"padding\": \"max_length\",\n","    \"return_tensors\": \"pt\",\n","    \"truncation\": True,\n","    \"add_special_tokens\": True,\n","     \"truncation_strategy\":\"longest_first\"\n","}"]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0dc62c58638743698718e12943baa4ff","5b4547e87d324c0eaa4a2a0b6c3897cf","dadabc1c0c8b437eaef1025c8b7c08aa","59b5d4a2e31c47aaad72f69b9275716c","b9f480e3bf5740348b7cf6377d4ad4e5","550b36ab7f6344c39438263b2a3d5c20","91e21986d768413a91714dbb5441a385","da57f6aa96f04c8cabd6f2f9003da959","a835b56eb3a84e89a2dd28d8517f8882","173362158da84492a89b2fa176d223ff","4ce1d8f7fb2a42d29a588bf65f19d506","1f811105fdde4ad4afdcc7dbd5560f51","d74c3af1e9614425ac0c4e647d5b5623","a1ab819d8aa942d399a808370f8e859b","766bf14d986b4524be24c4aa4ef8d4e1","e5456f8859004298be70e62f2e487067","73e25aff3147471b9bed7050520ee75d","8de4dccebc2c46f09555923c27587c8b","5e4c675884a448539eab86854153fa00","c5cedb4c2bf2498eb54bb6c299685b28","6f68a3e705c34f309959db13e66c895a","98489ac9215a4305a174fb216ff06105","f075962b09ff490a9b8c7374471cb3be","a26e46dfcbf54b8282457fc9d8b48517","c68be29bdbb248f98e7a06920b6d2be1","ddc47485057f4aa996f9056c15ad14ae","1f84f458df3f47fb90cfed73d88896fa","c15d513214b040abb4657c66f0aa78ed","926d6a1d8ec648578876ea713aadc9a2","baf4d2dc8a77432c9ea60fa391763118","412812beca3845369cd53f44e3398da3","7f77692d04d94311ba77d7316c1aca30","178865cd2a8a4c3f91c36c6e99729599","322621a7478d48dca8df58c0c84feb76","8a0f70f01cbf4cf4b6db135f97f276eb","f4b5ea26b2ad4077945c09dbdc209966","f26fbaead89f4ce6aa33df8df14215cf","36c012492e5d4226a0d632ab0c804c5a","e52b7ca5c6bf4c0a9418c42910d35c30","b1ea29831818444c9b58c8377dd0bf19","f01fa4cc149d435dbb1dbefbc82093a7","778664b063594ee1aea389d706bea73b","a664f858319b4771b8d50c490e43a84a","18fa412724da465f98548012c076b3a1","000992cded364d9db0969e6961a0c731","4743441c368b4ea2a042d6d083375f18","329fd7d11883491ea7177c80c7218d90","dbed8cdf72944b6d86dab5bc5ffaecb0","d85f09b82ce642c683eef550cb37da9a","37f60c92856e49f98b5712a86dab89b9","1c711621094b494682ec6ee2d39e1e98","6bf1f5b5599c4ed7a8330dd6381c45bf","81fa108f2b1348eb80da0583de48e175","b57cb7853591443d807cbe5c096577d4","59b2e1fa3a5b4bc28f9e42574b972f0e","fabd6549d2ac4e879d09220a526bdeea","b5af03dd12834e0fa34f7ad6ce4c1526","892f9df880354e1e954f7a2c3119392f","09076022acca4153bd62329c4b3ab9a6","fad218abe58140e6b8170e3acd9610f2","1a30e47d7b4e4f6cbf8a40730f06a9e9","d0b5664d6eb145b69958d8a6f56ef5b2","b48f5ee20b3e45019ec1aa9a6b4198f4","0e490b0757234169b38e4c737d25c23f","2157a50c06e2492c91f08edf76489190","5b23e471d87f4a02b029d78dc4b61e36"]},"id":"petGI7zd4LAm","executionInfo":{"status":"error","timestamp":1692341841308,"user_tz":-360,"elapsed":4754258,"user":{"displayName":"Shrestha Datta11","userId":"04875103577606312742"}},"outputId":"80f6fea1-aade-466c-8649-d42ff6754873"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc62c58638743698718e12943baa4ff"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\tTrain loss:0.989762.. \tValid Loss:0.846509.. \tVal Accuracy: 63.6842\n","\tLabel 0 Precision: 0.7016\tLabel 0 Recall: 0.7936\tLabel 0 F1-score: 0.7448\n","\tLabel 1 Precision: 0.5924\tLabel 1 Recall: 0.5228\tLabel 1 F1-score: 0.5554\n","\tLabel 2 Precision: 0.3974\tLabel 2 Recall: 0.3061\tLabel 2 F1-score: 0.3458\n","\tCombined F1-score: 0.5487\n","micro precision: 0.6368421052631579, Micro recall: 0.6368421052631579, micro f1: 0.6368421052631579\n","Epoch: 2/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f811105fdde4ad4afdcc7dbd5560f51"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["saved on epoch: 2\n","\tTrain loss:0.827664.. \tValid Loss:0.784231.. \tVal Accuracy: 67.2932\n","\tLabel 0 Precision: 0.7044\tLabel 0 Recall: 0.8410\tLabel 0 F1-score: 0.7667\n","\tLabel 1 Precision: 0.6096\tLabel 1 Recall: 0.5803\tLabel 1 F1-score: 0.5946\n","\tLabel 2 Precision: 0.6494\tLabel 2 Recall: 0.2551\tLabel 2 F1-score: 0.3663\n","\tCombined F1-score: 0.5759\n","micro precision: 0.6729323308270677, Micro recall: 0.6729323308270677, micro f1: 0.6729323308270677\n","Epoch: 3/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f075962b09ff490a9b8c7374471cb3be"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["saved on epoch: 3\n","\tTrain loss:0.750106.. \tValid Loss:0.829623.. \tVal Accuracy: 64.3609\n","\tLabel 0 Precision: 0.7458\tLabel 0 Recall: 0.7448\tLabel 0 F1-score: 0.7453\n","\tLabel 1 Precision: 0.6593\tLabel 1 Recall: 0.5012\tLabel 1 F1-score: 0.5695\n","\tLabel 2 Precision: 0.3805\tLabel 2 Recall: 0.5765\tLabel 2 F1-score: 0.4584\n","\tCombined F1-score: 0.5911\n","micro precision: 0.643609022556391, Micro recall: 0.643609022556391, micro f1: 0.643609022556391\n","Epoch: 4/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322621a7478d48dca8df58c0c84feb76"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\tTrain loss:0.707753.. \tValid Loss:0.802088.. \tVal Accuracy: 66.0902\n","\tLabel 0 Precision: 0.6785\tLabel 0 Recall: 0.9093\tLabel 0 F1-score: 0.7771\n","\tLabel 1 Precision: 0.8133\tLabel 1 Recall: 0.3237\tLabel 1 F1-score: 0.4631\n","\tLabel 2 Precision: 0.4532\tLabel 2 Recall: 0.4694\tLabel 2 F1-score: 0.4612\n","\tCombined F1-score: 0.5671\n","micro precision: 0.6609022556390978, Micro recall: 0.6609022556390978, micro f1: 0.6609022556390978\n","Epoch: 5/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"000992cded364d9db0969e6961a0c731"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["saved on epoch: 5\n","\tTrain loss:0.662405.. \tValid Loss:0.782501.. \tVal Accuracy: 68.1203\n","\tLabel 0 Precision: 0.7165\tLabel 0 Recall: 0.8354\tLabel 0 F1-score: 0.7714\n","\tLabel 1 Precision: 0.6698\tLabel 1 Recall: 0.5204\tLabel 1 F1-score: 0.5857\n","\tLabel 2 Precision: 0.5294\tLabel 2 Recall: 0.4592\tLabel 2 F1-score: 0.4918\n","\tCombined F1-score: 0.6163\n","micro precision: 0.681203007518797, Micro recall: 0.681203007518797, micro f1: 0.681203007518797\n","Epoch: 6/100\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1833 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fabd6549d2ac4e879d09220a526bdeea"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-39-a0960a318743>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Train Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Now Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-36-60b300edebbc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, config)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouped_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mforeach\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_has_foreach_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_foreach_mul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_coef_clamped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-overload]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mforeach\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'foreach=True was passed, but can\\'t use the foreach API on {device.type} tensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["train_loss_data, valid_loss_data = [], []\n","valid_loss_min = np.Inf\n","since = time.time()\n","best_loss = np.inf\n","best_acc=0\n","sml = 1e-10\n","best_f1=0.56\n","\n","for epoch in range(epochs):\n","\n","\n","    if epoch==1:\n","      training_data = NewsDatasets(df_train)\n","      train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n","    # monitor training loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    total = 0\n","    correct = 0\n","    label_0_TP = 0\n","    label_0_TN = 0\n","    label_0_FP = 0\n","    label_0_FN = 0\n","\n","    label_1_TP = 0\n","    label_1_TN = 0\n","    label_1_FP = 0\n","    label_1_FN = 0\n","\n","    label_2_TP = 0\n","    label_2_TN = 0\n","    label_2_FP = 0\n","    label_2_FN = 0\n","\n","\n","    e_since = time.time()\n","\n","    # Train Model\n","    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n","    # Now Evaluate\n","    out = evaluate(model, val_dataloader, criterion, tokenizer_config)\n","    total += out[0]\n","    correct += out[1]\n","    valid_loss += out[2]\n","    label_0_TP += out[3]\n","    label_0_TN += out[4]\n","    label_0_FP += out[5]\n","    label_0_FN += out[6]\n","\n","    label_1_TP += out[7]\n","    label_1_TN += out[8]\n","    label_1_FP += out[9]\n","    label_1_FN += out[10]\n","\n","    label_2_TP += out[11]\n","    label_2_TN += out[12]\n","    label_2_FP += out[13]\n","    label_2_FN += out[14]\n","\n","    # Calculate precision, recall, and F1-score for each class\n","    label_0_precision = label_0_TP / (label_0_TP + label_0_FP+sml)\n","    label_0_recall = label_0_TP / (label_0_TP + label_0_FN+sml)\n","    label_0_f1_score = 2 * (label_0_precision * label_0_recall) / (label_0_precision + label_0_recall+sml)\n","\n","    label_1_precision = label_1_TP / (label_1_TP + label_1_FP+sml)\n","    label_1_recall = label_1_TP / (label_1_TP + label_1_FN+sml)\n","    label_1_f1_score = 2 * (label_1_precision * label_1_recall) / (label_1_precision + label_1_recall+sml)\n","\n","    label_2_precision = label_2_TP / (label_2_TP + label_2_FP+sml)\n","    label_2_recall = label_2_TP / (label_2_TP + label_2_FN+sml)\n","    label_2_f1_score = 2 * (label_2_precision * label_2_recall) / (label_2_precision + label_2_recall+sml)\n","\n","    # Calculate combined F1-score\n","    combined_f1_score = (label_0_f1_score + label_1_f1_score + label_2_f1_score) / 3\n","\n","    # Calculate micro TP, TN, FP, FN values\n","    micro_TP = label_0_TP + label_1_TP + label_2_TP\n","    micro_TN = label_0_TN + label_1_TN + label_2_TN\n","    micro_FP = label_0_FP + label_1_FP + label_2_FP\n","    micro_FN = label_0_FN + label_1_FN + label_2_FN\n","\n","    # Calculate micro precision, recall, and F1 score\n","    micro_precision = micro_TP / (micro_TP + micro_FP)\n","    micro_recall = micro_TP / (micro_TP + micro_FN)\n","    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n","\n","    scheduler.step()\n","\n","    # print training/validation statistics\n","    # calculate average loss over an epoch\n","    train_loss = train_loss / len(train_dataloader.dataset)\n","    valid_loss = valid_loss / len(val_dataloader.dataset)\n","\n","    val_acc=correct / total * 100\n","\n","    # calculate train loss and running loss\n","    train_loss_data.append(train_loss * 100)\n","    valid_loss_data.append(valid_loss * 100)\n","\n","    if combined_f1_score > best_f1:\n","        best_f1 = combined_f1_score\n","        torch.save(model.state_dict(), DirPath+'Models by Sami/'+bert_model_name+\"_lasttwopooler_fromcollectedcontest_nofreeze_f1.pth\")\n","        print(f'saved on epoch: {epoch+1}')\n","\n","    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n","          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n","          \"\\tVal Accuracy: {:.4f}\".format(correct / total * 100))\n","    print(\"\\tLabel 0 Precision: {:.4f}\\tLabel 0 Recall: {:.4f}\\tLabel 0 F1-score: {:.4f}\\n\"\n","      \"\\tLabel 1 Precision: {:.4f}\\tLabel 1 Recall: {:.4f}\\tLabel 1 F1-score: {:.4f}\\n\"\n","      \"\\tLabel 2 Precision: {:.4f}\\tLabel 2 Recall: {:.4f}\\tLabel 2 F1-score: {:.4f}\\n\"\n","      \"\\tCombined F1-score: {:.4f}\".format(label_0_precision, label_0_recall, label_0_f1_score,\n","                                            label_1_precision, label_1_recall, label_1_f1_score,\n","                                            label_2_precision, label_2_recall, label_2_f1_score,\n","                                            combined_f1_score))\n","    print(f'micro precision: {micro_precision}, Micro recall: {micro_recall}, micro f1: {micro_f1}')\n","\n","time_elapsed = time.time() - since\n","print('Training completed in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muKYec9IECUQ"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcnJ4S_u65BO"},"outputs":[],"source":["torch.save(model.state_dict(), DirPath+bert_model_name+\"_lasttwopoolerf_contest_val_from_finalhs_midnonfrozen_acc1_sub_finaluntested.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYYhjA944Qgi"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","plt.plot(train_loss_data, label=\"Training loss\")\n","plt.plot(valid_loss_data, label=\"validation loss\")\n","plt.legend(frameon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d8iU2p64rrh"},"outputs":[],"source":["model.load_state_dict(torch.load(DirPath+bert_model_name+\"_lasttwopooler_contest_val.pth\", map_location = device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljsBQxEU48JF"},"outputs":[],"source":["all_preds = []\n","all_labels = []\n","\n","for batch in test_dataloader:\n","    text, labels = batch\n","    inputs = tokenizer.batch_encode_plus(\n","        text, **tokenizer_config\n","    )\n","    input_ids = inputs['input_ids'].to(device)\n","    token_type_ids = inputs['token_type_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","    labels = labels.to(device)\n","\n","    # move things to model\n","    output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","    preds = output.detach().cpu().numpy()\n","    preds = np.argmax(preds, axis = 1)\n","    all_preds.extend(preds)\n","    all_labels.extend(labels.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KxDxkk24918"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","# preds = np.argmax(preds, axis = 1)\n","print(classification_report(all_labels, all_preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMPUT4Qn8DZr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gZrljC9YjCNQ"},"source":["<h1>Training the model with All Collected dataset with the selected model and hyperparameters(Code not yet updated)<h1>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0qev8l4jq6O"},"outputs":[],"source":["!pip install --quiet transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ThJXgxvj3gH"},"outputs":[],"source":["import time\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from tqdm.notebook import tqdm\n","from transformers import BertModel, BertTokenizer, BertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xdZUKdxkCPv"},"outputs":[],"source":["#df loading\n","df_train = pd.read_csv('train.csv')[['sentence','hate speech']]\n","df_val = pd.read_csv('val.csv')[['sentence','hate speech']]\n","df_test = pd.read_csv('test.csv')[['sentence','hate speech']]\n","\n","#concatenating all the data\n","df_train = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","\n","print(df_train.shape)\n","print(df_val.shape)\n","print(df_test.shape)\n","print(df_train)\n","print(df_train.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AV03rC3pk1QX"},"outputs":[],"source":["#defining previous hyperparameters got from testing\n","max_number_input_tokens=256\n","batch_size_training = 16\n","first_dropout_rate = 0.3\n","hidden_output = 768\n","bert_model_name = \"sagorsarker/bangla-bert-base\"\n","adam_opt_lr = 3e-5\n","scheduler_step = 1\n","scheduler_gamma = 0.8\n","epochs = 6\n","classes = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miIj9oCklCtV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DirPath = ('/content/drive/My Drive/Test/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDDFZzD2lJhK"},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['sentence'] , value['hate speech']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GX135RhClpa7"},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6UhqDxyl0ch"},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 3, classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        o1 = outputs.hidden_states[-1]\n","        o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        pooled_output = o2\n","        cat = torch.cat((apool, mpool, pooled_output), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iSQ7Ub4l60H"},"outputs":[],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDznOw8Ol8mV"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CustomBERTBengali(bert)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2Nyc8fTmXl3"},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","\n","optimizer = AdamW(model.parameters(), lr=adam_opt_lr)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufV3a1Z8mdGH"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, config):\n","    model.train()  # prep model for training\n","    train_loss = 0\n","    for batch in tqdm(dataloader):\n","        text, labels = batch\n","\n","        model.zero_grad()\n","\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        #labels = labels.to(device)\n","        labels = labels.to(device, dtype=torch.long)  # Convert labels to torch.long\n","\n","        # move things to model\n","        logs = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss = criterion(logs, labels)\n","        train_loss += loss.item() * input_ids.size(0)\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iw0YDvjkmnHw"},"outputs":[],"source":["def evaluate(model, dataloader, criterion, config):\n","    total = 0\n","    correct = 0\n","    valid_loss = 0.0\n","\n","    model.eval()  # prep model for evaluation\n","    for batch in dataloader:\n","        text, labels = batch\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        labels = labels.to(device)\n","\n","        # move things to model\n","        output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss_p = criterion(output, labels)\n","        # update running validation loss\n","        valid_loss += loss_p.item() * input_ids.size(0)\n","        # calculate accuracy\n","        proba = torch.exp(output)\n","        top_p, top_class = proba.topk(1, dim=1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    return total, correct, valid_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foLVh7k6msA3"},"outputs":[],"source":["tokenizer_config = {\n","    \"max_length\": max_number_input_tokens,\n","    \"padding\": \"max_length\",\n","    \"return_tensors\": \"pt\",\n","    \"truncation\": True,\n","    \"add_special_tokens\": True,\n","     \"truncation_strategy\":\"longest_first\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcfncSwknBFV"},"outputs":[],"source":["train_loss_data, valid_loss_data = [], []\n","valid_loss_min = np.Inf\n","since = time.time()\n","best_loss = np.inf\n","\n","for epoch in range(epochs):\n","    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n","    # monitor training loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    total = 0\n","    correct = 0\n","    e_since = time.time()\n","\n","    # Train Model\n","    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n","    # Now Evaluate\n","    out = evaluate(model, val_dataloader, criterion, tokenizer_config)\n","    total += out[0]\n","    correct += out[1]\n","    valid_loss += out[2]\n","\n","    scheduler.step()\n","\n","    # print training/validation statistics\n","    # calculate average loss over an epoch\n","    train_loss = train_loss / len(train_dataloader.dataset)\n","    valid_loss = valid_loss / len(val_dataloader.dataset)\n","\n","    # calculate train loss and running loss\n","    train_loss_data.append(train_loss * 100)\n","    valid_loss_data.append(valid_loss * 100)\n","\n","    if True:\n","        best_loss = valid_loss\n","        torch.save(model.state_dict(), DirPath+bert_model_name+\"_CustomBertBengaliFullDataset6epoch885044valacc.pth\")\n","        print(f'epoch: {epoch+1}')\n","\n","    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n","          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n","          \"\\tVal Accuracy: {:.4f}\".format(correct / total * 100))\n","\n","time_elapsed = time.time() - since\n","print('Training completed in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDrNVhFXor-F"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"85922646f6db472588d1663a9f2f2b01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b6650677071f44b3b9ffaf4bc1f02751","IPY_MODEL_3255649900b54afcbe7f1bd644b8b9ec","IPY_MODEL_649c6851315542988880d6db7ac4222d"],"layout":"IPY_MODEL_5aa4eabea5034523b23fd9a269e69624"}},"b6650677071f44b3b9ffaf4bc1f02751":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c36c4a1015f54be89cad12277d71ddcd","placeholder":"​","style":"IPY_MODEL_520fd9e2b9344d3eb21e8bd6f346b764","value":"Downloading (…)lve/main/config.json: 100%"}},"3255649900b54afcbe7f1bd644b8b9ec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16bdca32887b4be7a5f947b18a0cd997","max":615,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35aa0b29a8da4bb5a5229733a098158e","value":615}},"649c6851315542988880d6db7ac4222d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af07f82d928f4ce9ad3f2a2f16026e05","placeholder":"​","style":"IPY_MODEL_535265ad812f4382b38c61972d43c56b","value":" 615/615 [00:00&lt;00:00, 24.5kB/s]"}},"5aa4eabea5034523b23fd9a269e69624":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c36c4a1015f54be89cad12277d71ddcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"520fd9e2b9344d3eb21e8bd6f346b764":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16bdca32887b4be7a5f947b18a0cd997":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35aa0b29a8da4bb5a5229733a098158e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af07f82d928f4ce9ad3f2a2f16026e05":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535265ad812f4382b38c61972d43c56b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7f818a2b4794253b643e3cb3388a719":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f2d5becc6d7484f841c62a172113bf2","IPY_MODEL_1b9e8f19227247659601d53638f7d563","IPY_MODEL_44927cb92c0a4163bd310c91812a9b3e"],"layout":"IPY_MODEL_84ddc07964b049fa8003fa7e90eb2a25"}},"0f2d5becc6d7484f841c62a172113bf2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a40f982b19124c14b3599836ddfd21a9","placeholder":"​","style":"IPY_MODEL_10a8f9cb016849ed9757ed0f3418aa58","value":"Downloading model.safetensors: 100%"}},"1b9e8f19227247659601d53638f7d563":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c860a22e04af488dbfcd51363f35ae1c","max":1115567652,"min":0,"orientation":"horizontal","style":"IPY_MODEL_862b8f27216648fd9e83a9ead6826dc5","value":1115567652}},"44927cb92c0a4163bd310c91812a9b3e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66a9988006504e37a8c8aca355332cdb","placeholder":"​","style":"IPY_MODEL_6555f9a9066e4fcaace9cfbca3c074ab","value":" 1.12G/1.12G [00:06&lt;00:00, 193MB/s]"}},"84ddc07964b049fa8003fa7e90eb2a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a40f982b19124c14b3599836ddfd21a9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10a8f9cb016849ed9757ed0f3418aa58":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c860a22e04af488dbfcd51363f35ae1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"862b8f27216648fd9e83a9ead6826dc5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66a9988006504e37a8c8aca355332cdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6555f9a9066e4fcaace9cfbca3c074ab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"60e93f550d414e27ae56f6da3d030525":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_410b785a78e34c9a8c72d63284f7fc3d","IPY_MODEL_bc542539167548f4a1723773cc440bc8","IPY_MODEL_cf5923ecdb8b42d9a6ee84fbd7108a83"],"layout":"IPY_MODEL_80d5b23bcbe0409cab4b9badae153a25"}},"410b785a78e34c9a8c72d63284f7fc3d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a3052834b724bc18e1013b8d365d507","placeholder":"​","style":"IPY_MODEL_898a224d27f04b2491fd3ef1364fd134","value":"Downloading (…)tencepiece.bpe.model: 100%"}},"bc542539167548f4a1723773cc440bc8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fee262b8c0b4efeb1548b2606d0eda4","max":5069051,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1cc81e83864945418caf3c89e89edc79","value":5069051}},"cf5923ecdb8b42d9a6ee84fbd7108a83":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30c91f083e0f426aa92ebff944b5319b","placeholder":"​","style":"IPY_MODEL_67a5d4e4293349a994a8d9fe0b00b2f6","value":" 5.07M/5.07M [00:03&lt;00:00, 1.53MB/s]"}},"80d5b23bcbe0409cab4b9badae153a25":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6a3052834b724bc18e1013b8d365d507":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"898a224d27f04b2491fd3ef1364fd134":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fee262b8c0b4efeb1548b2606d0eda4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1cc81e83864945418caf3c89e89edc79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30c91f083e0f426aa92ebff944b5319b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"67a5d4e4293349a994a8d9fe0b00b2f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ac661561ac545c6a604585a41d5dfcf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1cd31355d60141599d835386fa9085e1","IPY_MODEL_048ce0f0cf1b4d8d97e80dc5b79dc31e","IPY_MODEL_58e97b093d09408aa6304ffeb6fdf5ea"],"layout":"IPY_MODEL_b6a6d70840f6406baaf57d77c805c594"}},"1cd31355d60141599d835386fa9085e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_81d1213a26954c8496da9d1fb4382039","placeholder":"​","style":"IPY_MODEL_45dc909ccb6d4f4ea731b8832a91f532","value":"Downloading (…)/main/tokenizer.json: 100%"}},"048ce0f0cf1b4d8d97e80dc5b79dc31e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e10517115efa4743ace023cf325a3f44","max":9096718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c2161403ded49e9b8c717ec46212561","value":9096718}},"58e97b093d09408aa6304ffeb6fdf5ea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8aae7a65501a4ea1a0c1aacadb9aa3bf","placeholder":"​","style":"IPY_MODEL_006e3d13275c4c7eb2aa0caaf4ff044d","value":" 9.10M/9.10M [00:00&lt;00:00, 10.6MB/s]"}},"b6a6d70840f6406baaf57d77c805c594":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81d1213a26954c8496da9d1fb4382039":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45dc909ccb6d4f4ea731b8832a91f532":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e10517115efa4743ace023cf325a3f44":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c2161403ded49e9b8c717ec46212561":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8aae7a65501a4ea1a0c1aacadb9aa3bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"006e3d13275c4c7eb2aa0caaf4ff044d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0dc62c58638743698718e12943baa4ff":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5b4547e87d324c0eaa4a2a0b6c3897cf","IPY_MODEL_dadabc1c0c8b437eaef1025c8b7c08aa","IPY_MODEL_59b5d4a2e31c47aaad72f69b9275716c"],"layout":"IPY_MODEL_b9f480e3bf5740348b7cf6377d4ad4e5"}},"5b4547e87d324c0eaa4a2a0b6c3897cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_550b36ab7f6344c39438263b2a3d5c20","placeholder":"​","style":"IPY_MODEL_91e21986d768413a91714dbb5441a385","value":"100%"}},"dadabc1c0c8b437eaef1025c8b7c08aa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_da57f6aa96f04c8cabd6f2f9003da959","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a835b56eb3a84e89a2dd28d8517f8882","value":1833}},"59b5d4a2e31c47aaad72f69b9275716c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_173362158da84492a89b2fa176d223ff","placeholder":"​","style":"IPY_MODEL_4ce1d8f7fb2a42d29a588bf65f19d506","value":" 1833/1833 [13:42&lt;00:00,  2.22it/s]"}},"b9f480e3bf5740348b7cf6377d4ad4e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"550b36ab7f6344c39438263b2a3d5c20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91e21986d768413a91714dbb5441a385":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da57f6aa96f04c8cabd6f2f9003da959":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a835b56eb3a84e89a2dd28d8517f8882":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"173362158da84492a89b2fa176d223ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ce1d8f7fb2a42d29a588bf65f19d506":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f811105fdde4ad4afdcc7dbd5560f51":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d74c3af1e9614425ac0c4e647d5b5623","IPY_MODEL_a1ab819d8aa942d399a808370f8e859b","IPY_MODEL_766bf14d986b4524be24c4aa4ef8d4e1"],"layout":"IPY_MODEL_e5456f8859004298be70e62f2e487067"}},"d74c3af1e9614425ac0c4e647d5b5623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73e25aff3147471b9bed7050520ee75d","placeholder":"​","style":"IPY_MODEL_8de4dccebc2c46f09555923c27587c8b","value":"100%"}},"a1ab819d8aa942d399a808370f8e859b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e4c675884a448539eab86854153fa00","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c5cedb4c2bf2498eb54bb6c299685b28","value":1833}},"766bf14d986b4524be24c4aa4ef8d4e1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f68a3e705c34f309959db13e66c895a","placeholder":"​","style":"IPY_MODEL_98489ac9215a4305a174fb216ff06105","value":" 1833/1833 [13:48&lt;00:00,  2.20it/s]"}},"e5456f8859004298be70e62f2e487067":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73e25aff3147471b9bed7050520ee75d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8de4dccebc2c46f09555923c27587c8b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e4c675884a448539eab86854153fa00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5cedb4c2bf2498eb54bb6c299685b28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6f68a3e705c34f309959db13e66c895a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98489ac9215a4305a174fb216ff06105":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f075962b09ff490a9b8c7374471cb3be":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a26e46dfcbf54b8282457fc9d8b48517","IPY_MODEL_c68be29bdbb248f98e7a06920b6d2be1","IPY_MODEL_ddc47485057f4aa996f9056c15ad14ae"],"layout":"IPY_MODEL_1f84f458df3f47fb90cfed73d88896fa"}},"a26e46dfcbf54b8282457fc9d8b48517":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c15d513214b040abb4657c66f0aa78ed","placeholder":"​","style":"IPY_MODEL_926d6a1d8ec648578876ea713aadc9a2","value":"100%"}},"c68be29bdbb248f98e7a06920b6d2be1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_baf4d2dc8a77432c9ea60fa391763118","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_412812beca3845369cd53f44e3398da3","value":1833}},"ddc47485057f4aa996f9056c15ad14ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f77692d04d94311ba77d7316c1aca30","placeholder":"​","style":"IPY_MODEL_178865cd2a8a4c3f91c36c6e99729599","value":" 1833/1833 [13:48&lt;00:00,  2.21it/s]"}},"1f84f458df3f47fb90cfed73d88896fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c15d513214b040abb4657c66f0aa78ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"926d6a1d8ec648578876ea713aadc9a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"baf4d2dc8a77432c9ea60fa391763118":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"412812beca3845369cd53f44e3398da3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7f77692d04d94311ba77d7316c1aca30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"178865cd2a8a4c3f91c36c6e99729599":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"322621a7478d48dca8df58c0c84feb76":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8a0f70f01cbf4cf4b6db135f97f276eb","IPY_MODEL_f4b5ea26b2ad4077945c09dbdc209966","IPY_MODEL_f26fbaead89f4ce6aa33df8df14215cf"],"layout":"IPY_MODEL_36c012492e5d4226a0d632ab0c804c5a"}},"8a0f70f01cbf4cf4b6db135f97f276eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e52b7ca5c6bf4c0a9418c42910d35c30","placeholder":"​","style":"IPY_MODEL_b1ea29831818444c9b58c8377dd0bf19","value":"100%"}},"f4b5ea26b2ad4077945c09dbdc209966":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f01fa4cc149d435dbb1dbefbc82093a7","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_778664b063594ee1aea389d706bea73b","value":1833}},"f26fbaead89f4ce6aa33df8df14215cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a664f858319b4771b8d50c490e43a84a","placeholder":"​","style":"IPY_MODEL_18fa412724da465f98548012c076b3a1","value":" 1833/1833 [13:47&lt;00:00,  2.22it/s]"}},"36c012492e5d4226a0d632ab0c804c5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e52b7ca5c6bf4c0a9418c42910d35c30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1ea29831818444c9b58c8377dd0bf19":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f01fa4cc149d435dbb1dbefbc82093a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"778664b063594ee1aea389d706bea73b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a664f858319b4771b8d50c490e43a84a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18fa412724da465f98548012c076b3a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"000992cded364d9db0969e6961a0c731":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4743441c368b4ea2a042d6d083375f18","IPY_MODEL_329fd7d11883491ea7177c80c7218d90","IPY_MODEL_dbed8cdf72944b6d86dab5bc5ffaecb0"],"layout":"IPY_MODEL_d85f09b82ce642c683eef550cb37da9a"}},"4743441c368b4ea2a042d6d083375f18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_37f60c92856e49f98b5712a86dab89b9","placeholder":"​","style":"IPY_MODEL_1c711621094b494682ec6ee2d39e1e98","value":"100%"}},"329fd7d11883491ea7177c80c7218d90":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6bf1f5b5599c4ed7a8330dd6381c45bf","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81fa108f2b1348eb80da0583de48e175","value":1833}},"dbed8cdf72944b6d86dab5bc5ffaecb0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b57cb7853591443d807cbe5c096577d4","placeholder":"​","style":"IPY_MODEL_59b2e1fa3a5b4bc28f9e42574b972f0e","value":" 1833/1833 [13:47&lt;00:00,  2.22it/s]"}},"d85f09b82ce642c683eef550cb37da9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37f60c92856e49f98b5712a86dab89b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c711621094b494682ec6ee2d39e1e98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6bf1f5b5599c4ed7a8330dd6381c45bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81fa108f2b1348eb80da0583de48e175":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b57cb7853591443d807cbe5c096577d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59b2e1fa3a5b4bc28f9e42574b972f0e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fabd6549d2ac4e879d09220a526bdeea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5af03dd12834e0fa34f7ad6ce4c1526","IPY_MODEL_892f9df880354e1e954f7a2c3119392f","IPY_MODEL_09076022acca4153bd62329c4b3ab9a6"],"layout":"IPY_MODEL_fad218abe58140e6b8170e3acd9610f2"}},"b5af03dd12834e0fa34f7ad6ce4c1526":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a30e47d7b4e4f6cbf8a40730f06a9e9","placeholder":"​","style":"IPY_MODEL_d0b5664d6eb145b69958d8a6f56ef5b2","value":" 59%"}},"892f9df880354e1e954f7a2c3119392f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_b48f5ee20b3e45019ec1aa9a6b4198f4","max":1833,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0e490b0757234169b38e4c737d25c23f","value":1088}},"09076022acca4153bd62329c4b3ab9a6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2157a50c06e2492c91f08edf76489190","placeholder":"​","style":"IPY_MODEL_5b23e471d87f4a02b029d78dc4b61e36","value":" 1088/1833 [08:11&lt;05:36,  2.21it/s]"}},"fad218abe58140e6b8170e3acd9610f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a30e47d7b4e4f6cbf8a40730f06a9e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d0b5664d6eb145b69958d8a6f56ef5b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b48f5ee20b3e45019ec1aa9a6b4198f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e490b0757234169b38e4c737d25c23f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2157a50c06e2492c91f08edf76489190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b23e471d87f4a02b029d78dc4b61e36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}