{"cells":[{"cell_type":"markdown","metadata":{"id":"9dBC0DJLIEKX"},"source":["\u003ch1\u003eExtracting base finetuned(with BD-SHS dataset) from the finetuned model(Did not run the code yet)\u003ch1\u003e"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14553,"status":"ok","timestamp":1692259606260,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"F3xXUMDVfRI7","outputId":"219f0959-fe52-4257-e4ed-d2e23a66e689"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install --quiet transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":5125,"status":"ok","timestamp":1692259611381,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"KJs3tsZEdtJP"},"outputs":[],"source":["import time\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from tqdm.notebook import tqdm\n","from transformers import BertModel, BertTokenizer, BertForSequenceClassification, AutoModelForMaskedLM, AutoTokenizer"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692259611381,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"OR9A--a-jmNA"},"outputs":[],"source":["#defining some hyperparameters\n","max_number_input_tokens=256\n","batch_size_training = 16\n","first_dropout_rate = 0.3\n","hidden_output = 768\n","bert_model_name = \"xlm-roberta-base\"\n","adam_opt_lr = 3e-5\n","scheduler_step = 1\n","scheduler_gamma = 0.8\n","epochs = 10\n","classes = 2\n","need_split_dataset=False"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38084,"status":"ok","timestamp":1692259649461,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"rXg4692CZ5cb","outputId":"94be2722-b644-410b-f03d-1443bf58baa7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DirPath = ('/content/drive/My Drive/Test/')\n","Finetuned_model_path = DirPath+bert_model_name+\"_CustomBertBengaliFullDataset6epoch885044valacc.pth\"\n","CollectedDatasetFileName = \"Final_data.csv\"\n","CollectedDatasetPath = DirPath+\"EMNLP/\"+CollectedDatasetFileName\n","SplittedTrainFileName = \"train.csv\"\n","SplittedValFileName = \"dev.csv\"\n","SplittedTrainDataPath = DirPath+\"EMNLP/\"+SplittedTrainFileName\n","SplittedValDataPath = DirPath+\"EMNLP/\"+SplittedValFileName"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692252521885,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"AEBojsPoiA_b"},"outputs":[],"source":["def interchange(df_train,pos,label):\n","  #setting the first sample to be with label '0'\n","  zero_index = df_train[df_train['label'] == label].index[0]\n","  first_index=pos\n","  # interchange the samples\n","  df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","  return df_train"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692252521885,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"EmUATD-1YehW"},"outputs":[],"source":["def balanceclasses(df_train):\n","  class_counts = df_train['label'].value_counts()\n","  min_count = class_counts.max()\n","\n","  # Create new DataFrames for each class with fewer samples\n","  new_dfs = []\n","  for label, count in class_counts.items():\n","    if count == min_count:\n","        continue\n","    df_label = df_train[df_train['label'] == label]\n","    num_copies = min_count // count\n","    new_df_label = pd.concat([df_label] * num_copies, ignore_index=True)\n","    new_df_label = new_df_label.head(min_count-count)\n","    #print(new_df_label.head(10))\n","    new_dfs.append(new_df_label)\n","\n","  # Concatenate the new DataFrames with the original DataFrame\n","  df_balanced = pd.concat([df_train] + new_dfs, ignore_index=True).sample(frac=1).reset_index(drop=True)\n","  return df_balanced"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2857,"status":"ok","timestamp":1692252524739,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"mXlHCNIrflUj","outputId":"c75db706-4cf0-4d4f-e3f6-4a532d5a1ee1"},"outputs":[{"name":"stdout","output_type":"stream","text":["(2700, 2)\n","(1330, 2)\n","Train label counts:\n"," 0    1389\n","1     922\n","2     389\n","Name: label, dtype: int64\n","Validation label counts:\n"," 0    717\n","1    417\n","2    196\n","Name: label, dtype: int64\n","\n"," after making copies:\n","1    1389\n","0    1389\n","2    1389\n","Name: label, dtype: int64\n","                                                   text  label\n","0     বাংলাদেশের সরকার এগুলা দেখে না, না কি দেখেও না...      0\n","1     এটাতো মজনু পাগলার মতো সাজানো হচ্ছে ঢাকা বিশ্বব...      1\n","2     হায় রে পাগল..... যাকে খুজতে এক শহর থেকে আরেক শ...      2\n","3     হাহাহাহহাহাহহহাহাহা ভাই আমারে কেউ বিষ দে খাইয়া...      0\n","4     সাংবাদিক তোর..চু***।শুধু হেফাজত নয় সারা দেশের ...      1\n","...                                                 ...    ...\n","4162  ভিডিও তে দেখা গেল প্রথমেই নিউমার্কেটের কিছু কর...      0\n","4163  মত প্রকাশের অধিকার কি বাংলাদেশের মানুষের আছে??...      0\n","4164  নিবাচন আসলেই কত মায়ের বুক খালি হয় আল্লাহ তুমি ...      2\n","4165  সব ছাত্ররা এক হও কুত্তার বাচ্চা অশিক্ষিতদের উচ...      2\n","4166                        এটা বন্ধ করতে হবে খুব দ্রুত      0\n","\n","[4167 rows x 2 columns]\n","                                                   text  label\n","0     পাডা পুতার মাঝখানে পরে সাধারণ ২ মানুষের জিবন শ...      0\n","1     করোনার চাপে অনেক কিছু বন্ধ ও অনেক বিধি নিষেধ ক...      0\n","2     সঠিক তদন্ত করতে হবে। বিচারের আওতায় আনতে হবে য...      0\n","3     যে লোকটা মারা গেছে তার কি হবে তার দায়ভার কে ন...      0\n","4     নিউ মার্কেট এবং গুলিস্থান মার্কেটের ব্যবসায়ীর...      1\n","...                                                 ...    ...\n","1325  নাটক টা সুন্দর ভাবে সাজিয়েছে আরো কত কিছু দেখত...      1\n","1326  নোংরা দেশ আর নোংরা জাতি হচ্ছে ভারত এঁরা কি বুঝ...      1\n","1327                        জে ছেলে মারা গেছে ওর কি হবে      0\n","1328                           এরাই নৈরাজ্য সৃষ্টি করছে      1\n","1329  ব্রাহ্মণবাড়িয়ার হত্যা হলে, হেডলাইনে ব্রাহ্মণবা...      0\n","\n","[1330 rows x 2 columns]\n"]}],"source":["# from sklearn.model_selection import train_test_split\n","\n","# #splitting the dataset and saving\n","# if need_split_dataset==True:\n","#   #dataset loading\n","#   df = pd.read_csv(CollectedDatasetPath)[ ['Text','label'] ]\n","#   print(f'df label counts\\n',df['label'].value_counts())\n","#   # check if there is any NaN value in the dataframe\n","#   print(f'null values: {df.isna().sum()}')\n","\n","#   #null indices\n","#   null_index = df.index[df.isna().any(axis=1)]\n","#   print(f'null indices: {null_index}')\n","\n","#   #dropping null values\n","#   df = df.dropna()\n","\n","#   df_train, df_val = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n","#   df_train.to_csv(SplittedTrainDataPath)\n","#   df_val.to_csv(SplittedValDataPath)\n","# else:\n","#   df_train = pd.read_csv(SplittedTrainDataPath)[ ['text','label'] ]\n","#   df_val = pd.read_csv(SplittedValDataPath)[ ['text','label'] ]\n","\n","# # df_train = pd.concat([df_train, df_val], ignore_index=True)\n","# # df_val = df_train\n","\n","# # count the number of each unique label in train and validation dataframes\n","# train_label_counts = df_train['label'].value_counts()\n","# val_label_counts = df_val['label'].value_counts()\n","\n","# #setting the first sample to be with label '0'\n","# zero_index = df_train[df_train['label'] == 0].index[0]\n","# first_index=0\n","# # interchange the samples\n","# df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","# print(df_train.shape)\n","# print(df_val.shape)\n","# print('Train label counts:\\n', train_label_counts)\n","# print('Validation label counts:\\n', val_label_counts)\n","\n","# print(\"\\n after making copies:\")\n","# #balance all classes making copies\n","# df_train = balanceclasses(df_train)\n","# print(df_train['label'].value_counts())\n","\n","# #setting the first sample to be with label '0'\n","# zero_index = df_train[df_train['label'] == 0].index[0]\n","# first_index=0\n","# # interchange the samples\n","# df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","# #setting the first sample to be with label '0'\n","# zero_index = df_train[df_train['label'] == 1].index[0]\n","# first_index=1\n","# # interchange the samples\n","# df_train.iloc[[first_index, zero_index]] = df_train.iloc[[zero_index, first_index]]\n","\n","# print(df_train)\n","# print(df_val)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1692259851356,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"Z-k2B2ThB8SY","outputId":"9149c636-94ca-422d-de35-d079cdf3a42f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(50281, 2)\n","(5028, 2)\n","(5029, 2)\n","                                                sentence  hate speech\n","0                             .... ঐ ইন্দুর তোই মরছ নাই?            1\n","1      #গেবনের শেষে আইসা আপনার মুখোশ টা খুলছে এতেই আম...            1\n","2            ✈✈✈✈��� মুরগি চোরের পাছায় ডুকবি আর মারবি।।।            1\n","3      ১৮ কোটির চোদা খাওয়া শেষে এখন ১৫০কোটির চোদা খাব...            1\n","4                          ২য় মীর জাফরের মুখে মুতে দে...            1\n","...                                                  ...          ...\n","50276  হেট লাইন কি দিছোফালতু হেট লাইন দেয়ার জন্যআইন অ...            0\n","50277  হ্যা অবশ্যই পারবে শাকিব খান যেমন সুন্দর স্মার্...            0\n","50278                  হ্যাঁ ক্ষমা করে দিয়েছি রানু দিকে            0\n","50279  হ্যাঁ পমিতের যাব যেন শাকিব খানের উপজেলা এক হয়...            0\n","50280  হ্যালো আসসালামু আলাইকুম আল্লাহ তুমি মুসলমানদের...            0\n","\n","[50281 rows x 2 columns]\n","        hate speech\n","count  50281.000000\n","mean       0.480420\n","std        0.499621\n","min        0.000000\n","25%        0.000000\n","50%        0.000000\n","75%        1.000000\n","max        1.000000\n"]}],"source":["#df loading\n","df_train = pd.read_csv('train.csv')[['sentence','hate speech']]\n","df_val = pd.read_csv('val.csv')[['sentence','hate speech']]\n","df_test = pd.read_csv('test.csv')[['sentence','hate speech']]\n","\n","#concatenating all the data\n","df_train = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","\n","print(df_train.shape)\n","print(df_val.shape)\n","print(df_test.shape)\n","print(df_train)\n","print(df_train.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYqqzO0Qfa3t"},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['text'] , value['label']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vA643oD4ms6K"},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_val)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EteIRQlRp2vw"},"outputs":[],"source":["class HateSpeechBert(nn.Module):\n","\n","    def __init__(self, bert):\n","        super(HateSpeechBert, self).__init__()\n","\n","        self.bert = bert\n","\n","        # dropout layer\n","        self.dropout = nn.Dropout(first_dropout_rate)\n","\n","        # relu activation function\n","        self.relu = nn.ReLU()\n","        self.tanh = nn.Tanh()\n","\n","        # dense layer 1\n","        self.fc1 = nn.Linear(hidden_output*2, hidden_output)\n","\n","        #dense layer 2\n","        self.fc2 = nn.Linear(hidden_output, 128)\n","\n","        # dense layer 2 (Output layer)\n","        self.fc3 = nn.Linear(128, 2)\n","\n","        #softmax\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    # define the forward pass\n","    def forward(self, input_ids, token_type_ids, attention_mask):\n","        # pass the inputs to the model\n","        out = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","\n","        mean, _ = torch.max(out[0], 1)\n","        x= torch.cat((mean,out[1]), dim=1)\n","\n","        x = self.dropout(x)\n","\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","\n","        # output layer\n","        x = self.fc3(x)\n","        x = self.softmax(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVea3LVdlfpt"},"outputs":[],"source":["class BERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengali, self).__init__()\n","        #self.bert = BertForMaskedLM.from_pretrained(\"sagorsarker/bangla-bert-base\")\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(0.2)\n","        self.out = nn.Linear(hidden_output, 2)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        output = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        bo = self.bert_drop(output[1])\n","\n","        output = self.out(bo)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w7VapuTb3lzo"},"outputs":[],"source":["class BERTBengaliPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliPooler, self).__init__()\n","        self.bert = bert\n","        #self.bert.pooler.dense = nn.Linear(bert.config.hidden_size, bert.config.hidden_size)\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.out = nn.Linear(bert.config.hidden_size, classes)\n","        #softmax\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        pooled_output = outputs.pooler_output\n","        bo = self.bert_drop(pooled_output)\n","\n","        output = self.out(bo)\n","        output = self.softmax(bo)\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWAAuIfnn-nI"},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 3, classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        o1 = outputs.hidden_states[-1]\n","        o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        pooled_output = o2\n","        cat = torch.cat((apool, mpool, pooled_output), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNtrCwvch5nI"},"outputs":[],"source":["class BERTBengaliTwo(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliTwo, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 2, classes)\n","        torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        out = torch.cat((outputs.hidden_states[-1], outputs.hidden_states[-2]), dim=-1)\n","        out = self.drop_out(out)\n","        out = out[:,0,:]\n","        logits = self.l0(out)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EAFjVWK8SOkP"},"outputs":[],"source":["class BERTBengaliLastTwoPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPooler, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 3, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        #out = out[:,0,:]\n","        logits = self.l0(out)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":113},"executionInfo":{"elapsed":11195,"status":"ok","timestamp":1692084382499,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"X27bxqPFuaro","outputId":"0af77ba7-bd29-427a-c66a-3df47f98eb83"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"630ac8e3108049948202504132d36dcf","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/491 [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc5b0df0a56c4a028e49ac6e7c597234","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/660M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"83ae07045771402bbc465dfc6c33c12e","version_major":2,"version_minor":0},"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/2.24M [00:00\u003c?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YL9gaGTuocH"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","#creating the structure to contain finetuned bert\n","struct_model = CustomBERTBengali(bert)\n","struct_model.to(device)\n","\n","#loading the finetuned model which have leaned necessary info from other domain\n","struct_model.load_state_dict(torch.load(Finetuned_model_path))\n","\n","# Access the bert model\n","finetuned_bert_base = struct_model.bert"]},{"cell_type":"markdown","metadata":{"id":"nhygxywpTztv"},"source":["\u003ch1\u003eCreating a model with transferred learned knowledge capable of being finetuned with collected data and training it\n","##Note: test data and val data are same here.\u003ch1\u003e"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":648,"status":"ok","timestamp":1692259817887,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"7Q8iG3vluRLA"},"outputs":[],"source":["###Hyperparameter for the new model\n","#defining some hyperparameters\n","max_number_input_tokens=256\n","batch_size_training = 8\n","first_dropout_rate = 0.0\n","hidden_output = 768\n","bert_model_name = \"xlm-roberta-base\"\n","adam_opt_lr = 3e-5\n","scheduler_step = 1\n","scheduler_gamma = 0.98\n","epochs = 100\n","classes = 2\n","#need_split_dataset=False"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":432,"status":"ok","timestamp":1692260033783,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"MtIzKtM_xqHt"},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['sentence'] , value['hate speech']"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692260045664,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"aK9mkfgDxrVZ"},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_val)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSjqu8UTubQA"},"outputs":[],"source":["# #model for finetuning collected data\n","# class BERTBengaliLastTwoPooler(nn.Module):\n","#     def __init__(self, bert):\n","#         super(BERTBengaliLastTwoPooler, self).__init__()\n","#         self.bert = bert\n","#         self.drop_out = nn.Dropout(first_dropout_rate)\n","#         self.l0 =  nn.Linear(hidden_output * 3, classes)\n","#         #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","#         self.softmax = nn.Softmax(dim=1)\n","\n","#     def forward(self, input_ids, attention_mask, token_type_ids):\n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids\n","#         )\n","#         mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","#         out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","#         out = self.drop_out(out)\n","#         #out = out[:,0,:]\n","#         logits = self.l0(out)\n","#         logits = self.softmax(logits)\n","#         return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4_sM1dZ_Etmu"},"outputs":[],"source":["# class CustomBERTBengali(nn.Module):\n","#     def __init__(self, bert):\n","#         super(CustomBERTBengali, self).__init__()\n","#         self.bert = bert\n","#         self.bert_drop = nn.Dropout(first_dropout_rate)\n","#         self.tanh = nn.Tanh()\n","#         self.out = nn.Linear(hidden_output * 3, classes)\n","#         self.softmax = nn.Softmax(dim=1)\n","\n","#     def forward(self, input_ids, attention_mask, token_type_ids):\n","#         outputs = self.bert(\n","#             input_ids,\n","#             attention_mask=attention_mask,\n","#             token_type_ids=token_type_ids\n","#         )\n","#         o1 = outputs.hidden_states[-1]\n","#         o2 = outputs.pooler_output\n","#         apool = torch.mean(o1, 1)\n","#         mpool, _ = torch.max(o1, 1)\n","#         pooled_output = o2\n","#         cat = torch.cat((apool, mpool, pooled_output), 1)\n","#         bo = self.bert_drop(cat)\n","#         logits = self.out(bo)\n","#         #logits = self.softmax(logits)\n","#         return logits"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692260625013,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"BAH0JhR_gVdV"},"outputs":[],"source":["class BERTBengaliLastTwoPooler(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPooler, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l0 =  nn.Linear(hidden_output * 3, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((mpool, outputs.hidden_states[-2][:,0,:],outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        #out = out[:,0,:]\n","        logits = self.l0(out)\n","        # logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":510,"status":"ok","timestamp":1692252535979,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"Cf54Z8g_qqzb"},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 2, 2)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            # token_type_ids=token_type_ids\n","        )\n","        # print(dict(outputs).keys())\n","        o1 = outputs.hidden_states[-1]\n","        # o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        # pooled_output = o2\n","        cat = torch.cat((apool, mpool), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692252535979,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"W2Yk-_A3Y6OH"},"outputs":[],"source":["#model for finetuning collected data\n","class BERTBengaliLastTwoPoolerFreeze(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPoolerFreeze, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l2 = nn.Linear(hidden_output * 3, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l1 = nn.Linear(hidden_output * 2, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l0 = nn.Linear(hidden_output * 2, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((outputs.hidden_states[-2][:,0,:], mpool), dim=-1)#,outputs.pooler_output\n","        out = self.drop_out(out)\n","        out = self.l2(out)\n","        out = self.activation(out)\n","        out = self.l1(out)\n","        out = self.activation(out)\n","        logits = self.l0(out)\n","        #prob = self.softmax(logits)\n","        return logits\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692252539088,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"i4yc19NbQkJh"},"outputs":[],"source":["#model for finetuning collected data\n","class BERTBengaliLastTwoPoolerFreezePrev(nn.Module):\n","    def __init__(self, bert):\n","        super(BERTBengaliLastTwoPoolerFreezePrev, self).__init__()\n","        self.bert = bert\n","        self.drop_out = nn.Dropout(first_dropout_rate)\n","        self.l1 = nn.Linear(hidden_output * 2, hidden_output * 2)\n","        self.activation = nn.Tanh()\n","        self.l0 = nn.Linear(hidden_output * 2, classes)\n","        #torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        mpool, _ = torch.max(outputs.hidden_states[-1], 1)\n","        out = torch.cat((outputs.hidden_states[-2][:,0,:], mpool,outputs.pooler_output), dim=-1)\n","        out = self.drop_out(out)\n","        out = self.l1(out)\n","        out = self.activation(out)\n","        logits = self.l0(out)\n","        #prob = self.softmax(logits)\n","        return logits\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5720,"status":"ok","timestamp":1692260635842,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"9z9eXknsUPpV","outputId":"f0dbaa73-613d-4d33-cc31-3d8f7ecb9a42"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using a model of type xlm-roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","Some weights of BertModel were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['encoder.layer.0.attention.self.value.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.weight', 'pooler.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["name: bert.embeddings.word_embeddings.weight is trainable\n","name: bert.embeddings.position_embeddings.weight is trainable\n","name: bert.embeddings.token_type_embeddings.weight is trainable\n","name: bert.embeddings.LayerNorm.weight is trainable\n","name: bert.embeddings.LayerNorm.bias is trainable\n","name: bert.encoder.layer.0.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.0.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.0.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.0.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.0.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.0.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.0.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.0.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.0.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.0.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.0.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.0.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.0.output.dense.weight is non-trainable\n","name: bert.encoder.layer.0.output.dense.bias is non-trainable\n","name: bert.encoder.layer.0.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.0.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.1.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.1.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.1.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.1.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.1.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.1.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.1.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.1.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.1.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.1.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.1.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.1.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.1.output.dense.weight is non-trainable\n","name: bert.encoder.layer.1.output.dense.bias is non-trainable\n","name: bert.encoder.layer.1.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.1.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.2.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.2.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.2.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.2.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.2.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.2.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.2.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.2.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.2.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.2.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.2.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.2.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.2.output.dense.weight is non-trainable\n","name: bert.encoder.layer.2.output.dense.bias is non-trainable\n","name: bert.encoder.layer.2.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.2.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.3.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.3.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.3.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.3.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.3.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.3.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.3.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.3.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.3.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.3.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.3.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.3.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.3.output.dense.weight is non-trainable\n","name: bert.encoder.layer.3.output.dense.bias is non-trainable\n","name: bert.encoder.layer.3.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.3.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.4.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.4.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.4.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.4.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.4.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.4.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.4.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.4.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.4.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.4.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.4.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.4.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.4.output.dense.weight is non-trainable\n","name: bert.encoder.layer.4.output.dense.bias is non-trainable\n","name: bert.encoder.layer.4.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.4.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.5.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.5.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.5.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.5.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.5.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.5.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.5.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.5.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.5.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.5.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.5.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.5.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.5.output.dense.weight is non-trainable\n","name: bert.encoder.layer.5.output.dense.bias is non-trainable\n","name: bert.encoder.layer.5.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.5.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.6.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.6.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.6.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.6.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.6.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.6.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.6.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.6.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.6.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.6.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.6.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.6.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.6.output.dense.weight is non-trainable\n","name: bert.encoder.layer.6.output.dense.bias is non-trainable\n","name: bert.encoder.layer.6.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.6.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.7.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.7.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.7.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.7.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.7.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.7.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.7.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.7.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.7.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.7.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.7.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.7.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.7.output.dense.weight is non-trainable\n","name: bert.encoder.layer.7.output.dense.bias is non-trainable\n","name: bert.encoder.layer.7.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.7.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.8.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.8.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.8.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.8.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.8.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.8.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.8.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.8.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.8.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.8.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.8.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.8.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.8.output.dense.weight is non-trainable\n","name: bert.encoder.layer.8.output.dense.bias is non-trainable\n","name: bert.encoder.layer.8.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.8.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.9.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.9.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.9.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.9.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.9.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.9.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.9.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.9.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.9.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.9.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.9.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.9.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.9.output.dense.weight is non-trainable\n","name: bert.encoder.layer.9.output.dense.bias is non-trainable\n","name: bert.encoder.layer.9.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.9.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.10.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.10.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.10.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.10.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.10.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.10.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.10.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.10.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.10.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.10.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.10.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.10.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.10.output.dense.weight is non-trainable\n","name: bert.encoder.layer.10.output.dense.bias is non-trainable\n","name: bert.encoder.layer.10.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.10.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.11.attention.self.query.weight is non-trainable\n","name: bert.encoder.layer.11.attention.self.query.bias is non-trainable\n","name: bert.encoder.layer.11.attention.self.key.weight is non-trainable\n","name: bert.encoder.layer.11.attention.self.key.bias is non-trainable\n","name: bert.encoder.layer.11.attention.self.value.weight is non-trainable\n","name: bert.encoder.layer.11.attention.self.value.bias is non-trainable\n","name: bert.encoder.layer.11.attention.output.dense.weight is non-trainable\n","name: bert.encoder.layer.11.attention.output.dense.bias is non-trainable\n","name: bert.encoder.layer.11.attention.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.11.attention.output.LayerNorm.bias is non-trainable\n","name: bert.encoder.layer.11.intermediate.dense.weight is non-trainable\n","name: bert.encoder.layer.11.intermediate.dense.bias is non-trainable\n","name: bert.encoder.layer.11.output.dense.weight is non-trainable\n","name: bert.encoder.layer.11.output.dense.bias is non-trainable\n","name: bert.encoder.layer.11.output.LayerNorm.weight is non-trainable\n","name: bert.encoder.layer.11.output.LayerNorm.bias is non-trainable\n","name: bert.pooler.dense.weight is trainable\n","name: bert.pooler.dense.bias is trainable\n","name: l0.weight is trainable\n","name: l0.bias is trainable\n"]}],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","model = BERTBengaliLastTwoPooler(bert)\n","# model2Forlastlayers = CustomBERTBengali(bert)\n","\n","model.to(device)\n","# model2Forlastlayers.to(device)\n","# model2Forlastlayers.load_state_dict(torch.load(DirPath+'Models by Sami/'+bert_model_name+\"_modeltest.pth\"))\n","\n","# model.l0 = model2Forlastlayers.l0\n","# model.l2 = model2Forlastlayers.l1\n","# model.bert = model2Forlastlayers.bert\n","\n","# model.load_state_dict(torch.load(DirPath+bert_model_name+\"_lasttwopoolerf_contest_val_from_HScollected_lastfrozen_acc1_sub.pth\"))\n","\n","for params in model.bert.parameters():\n","  params.requires_grad = True\n","for params in model.bert.encoder.parameters():\n","  params.requires_grad = False\n","# for params in model.l2.parameters():\n","#   params.requires_grad = True\n","# for params in model.l1.parameters():\n","#   params.requires_grad = True\n","for params in model.l0.parameters():\n","  params.requires_grad = True\n","\n","for name, param in model.named_parameters():\n","  if param.requires_grad:\n","      print(f\"name: {name} is trainable\")\n","  else:\n","      print(f\"name: {name} is non-trainable\")"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2910,"status":"ok","timestamp":1692260638750,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"OmRT5yjsvdDK","outputId":"b0d86cfa-fcf8-4a15-a213-2d60302b4e16"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([[     0,  21145,  38732,   2801, 144840,  60420,   2730,    125,    378,\n","            294,  21290,    268,      2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n","tensor([[-0.3820,  1.8605]], device='cuda:0', grad_fn=\u003cAddmmBackward0\u003e)\n"]}],"source":["#testing if the input of model works before starting training\n","s = \"আমি বাংলায় গান গাই। [SEP]\"\n","t = tokenizer.encode_plus(s, return_tensors=\"pt\").to(device)\n","print(t)\n","out = model(**t)\n","print(out)"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":430,"status":"ok","timestamp":1692260656954,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"ZaJ8Ro_CxcPQ"},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","\n","optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=adam_opt_lr)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)"]},{"cell_type":"code","execution_count":33,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692260944622,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"Q-YKSmNf1HrB"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, config):\n","    model.train()  # prep model for training\n","    train_loss = 0\n","    for batch in tqdm(dataloader):\n","        text, labels = batch\n","\n","        model.zero_grad()\n","\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        # token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        #labels = labels.to(device)\n","        labels = labels.to(device, dtype=torch.long)  # Convert labels to torch.long\n","\n","        # move things to model\n","        logs = model( input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss = criterion(logs, labels)\n","        #print(\"successfully calculated criterion in train!\")\n","        train_loss += loss.item() * input_ids.size(0)\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    return train_loss"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692260945082,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"WULrKYWT1jyv"},"outputs":[],"source":["def evaluate(model, dataloader, criterion, config):\n","    total = 0\n","    correct = 0\n","    valid_loss = 0.0\n","    label_0_TP = 0\n","    label_0_TN = 0\n","    label_0_FP = 0\n","    label_0_FN = 0\n","\n","    label_1_TP = 0\n","    label_1_TN = 0\n","    label_1_FP = 0\n","    label_1_FN = 0\n","\n","    label_2_TP = 0\n","    label_2_TN = 0\n","    label_2_FP = 0\n","    label_2_FN = 0\n","\n","    model.eval()  # prep model for evaluation\n","    for batch in dataloader:\n","        text, labels = batch\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        # token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        labels = labels.to(device, dtype=torch.long)\n","\n","        # move things to model\n","        output = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss_p = criterion(output, labels)\n","        # update running validation loss\n","        valid_loss += loss_p.item() * input_ids.size(0)\n","        # calculate accuracy\n","        proba = torch.exp(output)\n","        top_p, top_class = proba.topk(1, dim=1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","        #print(f'predicted: {predicted} labels: {labels}')\n","        label_0_TP += ((predicted == 0) \u0026 (labels == 0)).sum().item()\n","        label_0_TN += ((predicted != 0) \u0026 (labels != 0)).sum().item()\n","        label_0_FP += ((predicted == 0) \u0026 (labels != 0)).sum().item()\n","        label_0_FN += ((predicted != 0) \u0026 (labels == 0)).sum().item()\n","\n","        label_1_TP += ((predicted == 1) \u0026 (labels == 1)).sum().item()\n","        label_1_TN += ((predicted != 1) \u0026 (labels != 1)).sum().item()\n","        label_1_FP += ((predicted == 1) \u0026 (labels != 1)).sum().item()\n","        label_1_FN += ((predicted != 1) \u0026 (labels == 1)).sum().item()\n","\n","        label_2_TP += ((predicted == 2) \u0026 (labels == 2)).sum().item()\n","        label_2_TN += ((predicted != 2) \u0026 (labels != 2)).sum().item()\n","        label_2_FP += ((predicted == 2) \u0026 (labels != 2)).sum().item()\n","        label_2_FN += ((predicted != 2) \u0026 (labels == 2)).sum().item()\n","\n","    return total, correct, valid_loss, label_0_TP, label_0_TN, label_0_FP, label_0_FN, label_1_TP, label_1_TN, label_1_FP, label_1_FN, label_2_TP, label_2_TN, label_2_FP, label_2_FN\n"]},{"cell_type":"code","execution_count":35,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1692260946314,"user":{"displayName":"Shrestha Datta8","userId":"16549252629907261162"},"user_tz":-360},"id":"Jfz6yeuc1pUZ"},"outputs":[],"source":["\n","tokenizer_config = {\n","    \"max_length\": max_number_input_tokens,\n","    \"padding\": \"max_length\",\n","    \"return_tensors\": \"pt\",\n","    \"truncation\": True,\n","    \"add_special_tokens\": True,\n","     \"truncation_strategy\":\"longest_first\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":750},"id":"petGI7zd4LAm"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/100\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8f18552c2754059a6cb74c176e17478","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6286 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved on epoch: 1\n","\tTrain loss:0.420829.. \tValid Loss:0.354928.. \tVal Accuracy: 85.6603\n","\tLabel 0 Precision: 0.8153\tLabel 0 Recall: 0.9361\tLabel 0 F1-score: 0.8715\n","\tLabel 1 Precision: 0.9177\tLabel 1 Recall: 0.7707\tLabel 1 F1-score: 0.8378\n","\tLabel 2 Precision: 0.0000\tLabel 2 Recall: 0.0000\tLabel 2 F1-score: 0.0000\n","\tCombined F1-score: 0.5698\n","micro precision: 0.8566030230708035, Micro recall: 0.8566030230708035, micro f1: 0.8566030230708035\n","Epoch: 2/100\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faed5a3ed9fb44dcb1d4996371edd23c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6286 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved on epoch: 2\n","\tTrain loss:0.357452.. \tValid Loss:0.331254.. \tVal Accuracy: 86.9133\n","\tLabel 0 Precision: 0.9105\tLabel 0 Recall: 0.8296\tLabel 0 F1-score: 0.8682\n","\tLabel 1 Precision: 0.8319\tLabel 1 Recall: 0.9118\tLabel 1 F1-score: 0.8701\n","\tLabel 2 Precision: 0.0000\tLabel 2 Recall: 0.0000\tLabel 2 F1-score: 0.0000\n","\tCombined F1-score: 0.5794\n","micro precision: 0.8691328560063644, Micro recall: 0.8691328560063644, micro f1: 0.8691328560063644\n","Epoch: 3/100\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0690d9567a694a4fb3ec2abd5d930856","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6286 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved on epoch: 3\n","\tTrain loss:0.342584.. \tValid Loss:0.313166.. \tVal Accuracy: 87.2315\n","\tLabel 0 Precision: 0.9257\tLabel 0 Recall: 0.8201\tLabel 0 F1-score: 0.8697\n","\tLabel 1 Precision: 0.8268\tLabel 1 Recall: 0.9288\tLabel 1 F1-score: 0.8749\n","\tLabel 2 Precision: 0.0000\tLabel 2 Recall: 0.0000\tLabel 2 F1-score: 0.0000\n","\tCombined F1-score: 0.5815\n","micro precision: 0.8723150357995226, Micro recall: 0.8723150357995226, micro f1: 0.8723150357995226\n","Epoch: 4/100\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7dcfad8fa0ff478385ffa8ea9d738e8e","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6286 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["saved on epoch: 4\n","\tTrain loss:0.327388.. \tValid Loss:0.299115.. \tVal Accuracy: 88.7232\n","\tLabel 0 Precision: 0.8834\tLabel 0 Recall: 0.9020\tLabel 0 F1-score: 0.8926\n","\tLabel 1 Precision: 0.8916\tLabel 1 Recall: 0.8713\tLabel 1 F1-score: 0.8813\n","\tLabel 2 Precision: 0.0000\tLabel 2 Recall: 0.0000\tLabel 2 F1-score: 0.0000\n","\tCombined F1-score: 0.5913\n","micro precision: 0.8872315035799523, Micro recall: 0.8872315035799523, micro f1: 0.8872315035799523\n","Epoch: 5/100\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d65645bfa2174875898a8d092b828684","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/6286 [00:00\u003c?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_loss_data, valid_loss_data = [], []\n","valid_loss_min = np.Inf\n","since = time.time()\n","best_loss = np.inf\n","best_acc=0\n","sml = 1e-10\n","best_f1=0.0\n","\n","for epoch in range(epochs):\n","\n","\n","    if epoch==1:\n","      training_data = NewsDatasets(df_train)\n","      train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n","    # monitor training loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    total = 0\n","    correct = 0\n","    label_0_TP = 0\n","    label_0_TN = 0\n","    label_0_FP = 0\n","    label_0_FN = 0\n","\n","    label_1_TP = 0\n","    label_1_TN = 0\n","    label_1_FP = 0\n","    label_1_FN = 0\n","\n","    label_2_TP = 0\n","    label_2_TN = 0\n","    label_2_FP = 0\n","    label_2_FN = 0\n","\n","\n","    e_since = time.time()\n","\n","    # Train Model\n","    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n","    # Now Evaluate\n","    out = evaluate(model, val_dataloader, criterion, tokenizer_config)\n","    total += out[0]\n","    correct += out[1]\n","    valid_loss += out[2]\n","    label_0_TP += out[3]\n","    label_0_TN += out[4]\n","    label_0_FP += out[5]\n","    label_0_FN += out[6]\n","\n","    label_1_TP += out[7]\n","    label_1_TN += out[8]\n","    label_1_FP += out[9]\n","    label_1_FN += out[10]\n","\n","    label_2_TP += out[11]\n","    label_2_TN += out[12]\n","    label_2_FP += out[13]\n","    label_2_FN += out[14]\n","\n","    # Calculate precision, recall, and F1-score for each class\n","    label_0_precision = label_0_TP / (label_0_TP + label_0_FP+sml)\n","    label_0_recall = label_0_TP / (label_0_TP + label_0_FN+sml)\n","    label_0_f1_score = 2 * (label_0_precision * label_0_recall) / (label_0_precision + label_0_recall+sml)\n","\n","    label_1_precision = label_1_TP / (label_1_TP + label_1_FP+sml)\n","    label_1_recall = label_1_TP / (label_1_TP + label_1_FN+sml)\n","    label_1_f1_score = 2 * (label_1_precision * label_1_recall) / (label_1_precision + label_1_recall+sml)\n","\n","    label_2_precision = label_2_TP / (label_2_TP + label_2_FP+sml)\n","    label_2_recall = label_2_TP / (label_2_TP + label_2_FN+sml)\n","    label_2_f1_score = 2 * (label_2_precision * label_2_recall) / (label_2_precision + label_2_recall+sml)\n","\n","    # Calculate combined F1-score\n","    combined_f1_score = (label_0_f1_score + label_1_f1_score + label_2_f1_score) / 3\n","\n","    # Calculate micro TP, TN, FP, FN values\n","    micro_TP = label_0_TP + label_1_TP + label_2_TP\n","    micro_TN = label_0_TN + label_1_TN + label_2_TN\n","    micro_FP = label_0_FP + label_1_FP + label_2_FP\n","    micro_FN = label_0_FN + label_1_FN + label_2_FN\n","\n","    # Calculate micro precision, recall, and F1 score\n","    micro_precision = micro_TP / (micro_TP + micro_FP)\n","    micro_recall = micro_TP / (micro_TP + micro_FN)\n","    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall)\n","\n","    scheduler.step()\n","\n","    # print training/validation statistics\n","    # calculate average loss over an epoch\n","    train_loss = train_loss / len(train_dataloader.dataset)\n","    valid_loss = valid_loss / len(val_dataloader.dataset)\n","\n","    val_acc=correct / total * 100\n","\n","    # calculate train loss and running loss\n","    train_loss_data.append(train_loss * 100)\n","    valid_loss_data.append(valid_loss * 100)\n","\n","    if combined_f1_score \u003e best_f1:\n","        best_f1 = combined_f1_score\n","        torch.save(model.state_dict(), DirPath+'Models by Sami/'+bert_model_name+\"_lasttwopooler_fromHS_freezeencoder_f1.pth\")\n","        print(f'saved on epoch: {epoch+1}')\n","\n","    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n","          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n","          \"\\tVal Accuracy: {:.4f}\".format(correct / total * 100))\n","    print(\"\\tLabel 0 Precision: {:.4f}\\tLabel 0 Recall: {:.4f}\\tLabel 0 F1-score: {:.4f}\\n\"\n","      \"\\tLabel 1 Precision: {:.4f}\\tLabel 1 Recall: {:.4f}\\tLabel 1 F1-score: {:.4f}\\n\"\n","      \"\\tLabel 2 Precision: {:.4f}\\tLabel 2 Recall: {:.4f}\\tLabel 2 F1-score: {:.4f}\\n\"\n","      \"\\tCombined F1-score: {:.4f}\".format(label_0_precision, label_0_recall, label_0_f1_score,\n","                                            label_1_precision, label_1_recall, label_1_f1_score,\n","                                            label_2_precision, label_2_recall, label_2_f1_score,\n","                                            combined_f1_score))\n","    print(f'micro precision: {micro_precision}, Micro recall: {micro_recall}, micro f1: {micro_f1}')\n","\n","time_elapsed = time.time() - since\n","print('Training completed in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"muKYec9IECUQ"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gcnJ4S_u65BO"},"outputs":[],"source":["torch.save(model.state_dict(), DirPath+bert_model_name+\"_lasttwopoolerf_contest_val_from_finalhs_midnonfrozen_acc1_sub_finaluntested.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sYYhjA944Qgi"},"outputs":[],"source":["from matplotlib import pyplot as plt\n","\n","plt.plot(train_loss_data, label=\"Training loss\")\n","plt.plot(valid_loss_data, label=\"validation loss\")\n","plt.legend(frameon=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d8iU2p64rrh"},"outputs":[],"source":["model.load_state_dict(torch.load(DirPath+bert_model_name+\"_lasttwopooler_contest_val.pth\", map_location = device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ljsBQxEU48JF"},"outputs":[],"source":["all_preds = []\n","all_labels = []\n","\n","for batch in test_dataloader:\n","    text, labels = batch\n","    inputs = tokenizer.batch_encode_plus(\n","        text, **tokenizer_config\n","    )\n","    input_ids = inputs['input_ids'].to(device)\n","    token_type_ids = inputs['token_type_ids'].to(device)\n","    attention_mask = inputs['attention_mask'].to(device)\n","    labels = labels.to(device)\n","\n","    # move things to model\n","    output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","    preds = output.detach().cpu().numpy()\n","    preds = np.argmax(preds, axis = 1)\n","    all_preds.extend(preds)\n","    all_labels.extend(labels.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3KxDxkk24918"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","\n","# preds = np.argmax(preds, axis = 1)\n","print(classification_report(all_labels, all_preds))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMPUT4Qn8DZr"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"gZrljC9YjCNQ"},"source":["\u003ch1\u003eTraining the model with All Collected dataset with the selected model and hyperparameters(Code not yet updated)\u003ch1\u003e"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0qev8l4jq6O"},"outputs":[],"source":["!pip install --quiet transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ThJXgxvj3gH"},"outputs":[],"source":["import time\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from tqdm.notebook import tqdm\n","from transformers import BertModel, BertTokenizer, BertForSequenceClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xdZUKdxkCPv"},"outputs":[],"source":["#df loading\n","df_train = pd.read_csv('train.csv')[['sentence','hate speech']]\n","df_val = pd.read_csv('val.csv')[['sentence','hate speech']]\n","df_test = pd.read_csv('test.csv')[['sentence','hate speech']]\n","\n","#concatenating all the data\n","df_train = pd.concat([df_train, df_val, df_test], ignore_index=True)\n","\n","print(df_train.shape)\n","print(df_val.shape)\n","print(df_test.shape)\n","print(df_train)\n","print(df_train.describe())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AV03rC3pk1QX"},"outputs":[],"source":["#defining previous hyperparameters got from testing\n","max_number_input_tokens=256\n","batch_size_training = 16\n","first_dropout_rate = 0.3\n","hidden_output = 768\n","bert_model_name = \"sagorsarker/bangla-bert-base\"\n","adam_opt_lr = 3e-5\n","scheduler_step = 1\n","scheduler_gamma = 0.8\n","epochs = 6\n","classes = 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miIj9oCklCtV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","DirPath = ('/content/drive/My Drive/Test/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aDDFZzD2lJhK"},"outputs":[],"source":["class NewsDatasets(Dataset):\n","    def __init__(self, data, max_length=max_number_input_tokens):\n","        self.data = data\n","\n","        self.config = {\n","            \"max_length\": max_length,\n","            \"padding\": \"max_length\",\n","            \"return_tensors\": \"pt\",\n","            \"truncation\": True,\n","            \"add_special_tokens\": True,\n","            \"truncation_strategy\":\"longest_first\"\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        value = self.data.iloc[idx]\n","        return value['sentence'] , value['hate speech']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GX135RhClpa7"},"outputs":[],"source":["training_data = NewsDatasets(df_train)\n","train_dataloader = DataLoader(training_data, batch_size=batch_size_training, shuffle=True)\n","\n","val_data = NewsDatasets(df_val)\n","val_dataloader = DataLoader(val_data, batch_size=batch_size_training, shuffle=True)\n","\n","test_data = NewsDatasets(df_test)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size_training, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6UhqDxyl0ch"},"outputs":[],"source":["class CustomBERTBengali(nn.Module):\n","    def __init__(self, bert):\n","        super(CustomBERTBengali, self).__init__()\n","        self.bert = bert\n","        self.bert_drop = nn.Dropout(first_dropout_rate)\n","        self.tanh = nn.Tanh()\n","        self.out = nn.Linear(hidden_output * 3, classes)\n","        self.softmax = nn.Softmax(dim=1)\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids\n","        )\n","        o1 = outputs.hidden_states[-1]\n","        o2 = outputs.pooler_output\n","        apool = torch.mean(o1, 1)\n","        mpool, _ = torch.max(o1, 1)\n","        pooled_output = o2\n","        cat = torch.cat((apool, mpool, pooled_output), 1)\n","        bo = self.bert_drop(cat)\n","        logits = self.out(bo)\n","        logits = self.softmax(logits)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0iSQ7Ub4l60H"},"outputs":[],"source":["bert = BertModel.from_pretrained(bert_model_name, output_hidden_states=True)\n","tokenizer = BertTokenizer.from_pretrained(bert_model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDznOw8Ol8mV"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = CustomBERTBengali(bert)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2Nyc8fTmXl3"},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","\n","optimizer = AdamW(model.parameters(), lr=adam_opt_lr)\n","criterion = nn.CrossEntropyLoss()\n","scheduler = StepLR(optimizer, step_size=scheduler_step, gamma=scheduler_gamma)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ufV3a1Z8mdGH"},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, config):\n","    model.train()  # prep model for training\n","    train_loss = 0\n","    for batch in tqdm(dataloader):\n","        text, labels = batch\n","\n","        model.zero_grad()\n","\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        #labels = labels.to(device)\n","        labels = labels.to(device, dtype=torch.long)  # Convert labels to torch.long\n","\n","        # move things to model\n","        logs = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss = criterion(logs, labels)\n","        train_loss += loss.item() * input_ids.size(0)\n","        loss.backward()\n","\n","        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iw0YDvjkmnHw"},"outputs":[],"source":["def evaluate(model, dataloader, criterion, config):\n","    total = 0\n","    correct = 0\n","    valid_loss = 0.0\n","\n","    model.eval()  # prep model for evaluation\n","    for batch in dataloader:\n","        text, labels = batch\n","        inputs = tokenizer.batch_encode_plus(\n","            text, **config\n","        )\n","        input_ids = inputs['input_ids'].to(device)\n","        token_type_ids = inputs['token_type_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        labels = labels.to(device)\n","\n","        # move things to model\n","        output = model(token_type_ids=token_type_ids, input_ids=input_ids, attention_mask=attention_mask)\n","\n","        loss_p = criterion(output, labels)\n","        # update running validation loss\n","        valid_loss += loss_p.item() * input_ids.size(0)\n","        # calculate accuracy\n","        proba = torch.exp(output)\n","        top_p, top_class = proba.topk(1, dim=1)\n","        equals = top_class == labels.view(*top_class.shape)\n","        # accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n","\n","        _, predicted = torch.max(output.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    return total, correct, valid_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foLVh7k6msA3"},"outputs":[],"source":["tokenizer_config = {\n","    \"max_length\": max_number_input_tokens,\n","    \"padding\": \"max_length\",\n","    \"return_tensors\": \"pt\",\n","    \"truncation\": True,\n","    \"add_special_tokens\": True,\n","     \"truncation_strategy\":\"longest_first\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcfncSwknBFV"},"outputs":[],"source":["train_loss_data, valid_loss_data = [], []\n","valid_loss_min = np.Inf\n","since = time.time()\n","best_loss = np.inf\n","\n","for epoch in range(epochs):\n","    print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n","    # monitor training loss\n","    train_loss = 0.0\n","    valid_loss = 0.0\n","    total = 0\n","    correct = 0\n","    e_since = time.time()\n","\n","    # Train Model\n","    train_loss += train(model, train_dataloader, optimizer, criterion, tokenizer_config)\n","    # Now Evaluate\n","    out = evaluate(model, val_dataloader, criterion, tokenizer_config)\n","    total += out[0]\n","    correct += out[1]\n","    valid_loss += out[2]\n","\n","    scheduler.step()\n","\n","    # print training/validation statistics\n","    # calculate average loss over an epoch\n","    train_loss = train_loss / len(train_dataloader.dataset)\n","    valid_loss = valid_loss / len(val_dataloader.dataset)\n","\n","    # calculate train loss and running loss\n","    train_loss_data.append(train_loss * 100)\n","    valid_loss_data.append(valid_loss * 100)\n","\n","    if True:\n","        best_loss = valid_loss\n","        torch.save(model.state_dict(), DirPath+bert_model_name+\"_CustomBertBengaliFullDataset6epoch885044valacc.pth\")\n","        print(f'epoch: {epoch+1}')\n","\n","    print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n","          \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n","          \"\\tVal Accuracy: {:.4f}\".format(correct / total * 100))\n","\n","time_elapsed = time.time() - since\n","print('Training completed in {:.0f}m {:.0f}s'.format(\n","    time_elapsed // 60, time_elapsed % 60))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KDrNVhFXor-F"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"00bc13d4c8cb42aab73e5e7814ef067d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2bf1d6c4ca94b319e6775ac49281f61","placeholder":"​","style":"IPY_MODEL_cd48af32a436420ab9970987be0b5c89","value":"Downloading model.safetensors: 100%"}},"030eba548a934f4b8d82d38f7d8da9cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0690d9567a694a4fb3ec2abd5d930856":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_09ca9fad1d434771ac2aba610f0a1754","IPY_MODEL_8b37065489114868b96e48e02da3dd49","IPY_MODEL_bfd21a4df80c465e93f23ef948903be8"],"layout":"IPY_MODEL_5813cfca467e4dd6ad9df563d9434596"}},"0990afd5a4014249b9a7b4f7a2749d5f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09ca9fad1d434771ac2aba610f0a1754":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d254ef228cd4959a2a078919434cbb7","placeholder":"​","style":"IPY_MODEL_53c7ad282b5f44e0a5bc9ed4d01061cd","value":"100%"}},"0b508142981a42168614afab5d7e7dfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b71bf6934bc465ba633fc2542ea7b80","placeholder":"​","style":"IPY_MODEL_030eba548a934f4b8d82d38f7d8da9cd","value":"100%"}},"12557673ebb6491bba4003a8209fa5c3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16bd97625c364d36a3d56c6c7ad00fc8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_21627bb8336945c1a98036d310edb4dc","placeholder":"​","style":"IPY_MODEL_4fcc757aa2d84ab691339db85c94d382","value":" 6286/6286 [34:52\u0026lt;00:00,  3.68it/s]"}},"1ef200f3d0814749b7c14f3898711b7e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21627bb8336945c1a98036d310edb4dc":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26b99702b701400e9161090ecce228f6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2809c28fca494d6e83ed5f2f4927c27f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"28fceecac72c45fba2d6fc4ee4f007ad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ad0fc2f759844dcaecf25852016abf8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ef200f3d0814749b7c14f3898711b7e","placeholder":"​","style":"IPY_MODEL_f59ceea710674b4e9a98f2c68c162ef8","value":" 119/6286 [00:39\u0026lt;35:11,  2.92it/s]"}},"2bba495afd82436fb02cecbbe6332548":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_26b99702b701400e9161090ecce228f6","max":2237676,"min":0,"orientation":"horizontal","style":"IPY_MODEL_855c7b2c4c2a4754832469e317736ee1","value":2237676}},"2dff246368ab486b8a0ff0e6b39ca133":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32f41879190a4c219416ea9988abfdd5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4367b30aa00458da28a9aef1f705e81","placeholder":"​","style":"IPY_MODEL_facf646bb731412aa91c9147832990e8","value":" 2.24M/2.24M [00:00\u0026lt;00:00, 19.0MB/s]"}},"3b71bf6934bc465ba633fc2542ea7b80":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bfce859e23945caa6faab7e9ff0aab3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57b7053ca1654f65a7b56bee30689970","placeholder":"​","style":"IPY_MODEL_b31020446f8c4e9b87356a69afec59d7","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"3c458b7ad7634963ac36e2732300897b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d254ef228cd4959a2a078919434cbb7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ed954320a5040c3802f07deb361c63a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f46cba507164b7a9301d09e9ee9d346":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec110c936bb245f9833f5fc7855d7e5f","max":660393036,"min":0,"orientation":"horizontal","style":"IPY_MODEL_536c84624f9246f980fb19b4bafc7b5d","value":660393036}},"44b32f2402ad4f98954d2f16c5f43884":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e1bf5000c434a1cbf0c4e61730d16cf","max":6286,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b6c9ec085c9c40a98052ca33b3547b45","value":119}},"4fcc757aa2d84ab691339db85c94d382":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"536c84624f9246f980fb19b4bafc7b5d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53c7ad282b5f44e0a5bc9ed4d01061cd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"57b7053ca1654f65a7b56bee30689970":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5813cfca467e4dd6ad9df563d9434596":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"586a78e95bba45b7962291bc3ffca3af":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"611d320232694e838110ab24bb62fc87":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"630ac8e3108049948202504132d36dcf":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_968f758e0e0c4cd4beb57a36bdd1510a","IPY_MODEL_a89148df9a8f4a29ba695c4e83ae5a8f","IPY_MODEL_b5f1704c51c44e949555c1296a2618e8"],"layout":"IPY_MODEL_ddcea33b4b4045a5b9ee7fd6a45ec2e7"}},"67398c0a379b484eb66609ba9009601c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"674ba17ba284486284728ef077acd855":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c22f936863d473c914310d951c2d4b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6c904cef07554c65ae80204b8eb8403f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_879dd2c8bf174794a1e9bc60a054f29f","placeholder":"​","style":"IPY_MODEL_0990afd5a4014249b9a7b4f7a2749d5f","value":" 6286/6286 [34:54\u0026lt;00:00,  3.70it/s]"}},"6d78de2e1ded478eb3c1172dcc00965a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6e1bf5000c434a1cbf0c4e61730d16cf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2ca4629ac44e239c7307adb21176bb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7387da86f5b2420cb98f5b50e827c9b3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c458b7ad7634963ac36e2732300897b","max":6286,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c12be9636fac4deda2f6e64405fe0565","value":5966}},"75f0fd7c90fc4a589f76fd011be14d62":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7854239d00db48f2bbb0ab3cbff15f02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7dcfad8fa0ff478385ffa8ea9d738e8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0b508142981a42168614afab5d7e7dfc","IPY_MODEL_dcc8a8408c944fed8448d2a347614c05","IPY_MODEL_6c904cef07554c65ae80204b8eb8403f"],"layout":"IPY_MODEL_dd61074f6ab84781b11e41ff9c91a88c"}},"7fdb2626bc314522bbe75bdcca244511":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67398c0a379b484eb66609ba9009601c","placeholder":"​","style":"IPY_MODEL_28fceecac72c45fba2d6fc4ee4f007ad","value":"  2%"}},"83ae07045771402bbc465dfc6c33c12e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3bfce859e23945caa6faab7e9ff0aab3","IPY_MODEL_2bba495afd82436fb02cecbbe6332548","IPY_MODEL_32f41879190a4c219416ea9988abfdd5"],"layout":"IPY_MODEL_f2157328d29043ccbd7d663928b9eedf"}},"846b33783fdd42f89273ee7ca19b64ed":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"855c7b2c4c2a4754832469e317736ee1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"879dd2c8bf174794a1e9bc60a054f29f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a5dd0433f514c848747aa916aad4ea2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8ac7f9cc06d14c6c8040d0c23bf28daa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75f0fd7c90fc4a589f76fd011be14d62","placeholder":"​","style":"IPY_MODEL_2809c28fca494d6e83ed5f2f4927c27f","value":" 660M/660M [00:06\u0026lt;00:00, 108MB/s]"}},"8b37065489114868b96e48e02da3dd49":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_586a78e95bba45b7962291bc3ffca3af","max":6286,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0dd40db5e8849da980302ea7745b5bd","value":6286}},"8ebf0cd4f6a54fb0a904c7a85084b82b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0a19e438c7940bd99c360270e4f1ff7","max":6286,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a5dd0433f514c848747aa916aad4ea2","value":6286}},"968f758e0e0c4cd4beb57a36bdd1510a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbe7b270e4d44ff7898b1e0790745ab1","placeholder":"​","style":"IPY_MODEL_f2e912153e3744dbaf34ca07cad96eb5","value":"Downloading (…)lve/main/config.json: 100%"}},"a0dd40db5e8849da980302ea7745b5bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a4367b30aa00458da28a9aef1f705e81":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5e24ece9d15484aa7c6004b98a0e853":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a89148df9a8f4a29ba695c4e83ae5a8f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_846b33783fdd42f89273ee7ca19b64ed","max":491,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bac8025fdd134cc09fd231b1e3102ca7","value":491}},"a9d33074aa5a4dd2ace0200d0cdcfd4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_611d320232694e838110ab24bb62fc87","placeholder":"​","style":"IPY_MODEL_c9f742f9970945c79493fde95997387c","value":" 5966/6286 [33:06\u0026lt;01:46,  3.01it/s]"}},"b26fd7d0905b450fa5543ce7757495d2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b30c3a52219646728a630e4beadd1ff4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b31020446f8c4e9b87356a69afec59d7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5f1704c51c44e949555c1296a2618e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c5c08785782b48e08fffeca2be5cda59","placeholder":"​","style":"IPY_MODEL_6d78de2e1ded478eb3c1172dcc00965a","value":" 491/491 [00:00\u0026lt;00:00, 12.5kB/s]"}},"b6c9ec085c9c40a98052ca33b3547b45":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b8f18552c2754059a6cb74c176e17478":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f428a890581d4729b985f08af336461c","IPY_MODEL_7387da86f5b2420cb98f5b50e827c9b3","IPY_MODEL_a9d33074aa5a4dd2ace0200d0cdcfd4c"],"layout":"IPY_MODEL_7854239d00db48f2bbb0ab3cbff15f02"}},"bac8025fdd134cc09fd231b1e3102ca7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bbe7b270e4d44ff7898b1e0790745ab1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfd21a4df80c465e93f23ef948903be8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_de224dfbc0bf405ea0c40d10186214e2","placeholder":"​","style":"IPY_MODEL_6c22f936863d473c914310d951c2d4b3","value":" 6286/6286 [34:52\u0026lt;00:00,  3.72it/s]"}},"c12be9636fac4deda2f6e64405fe0565":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c2bf1d6c4ca94b319e6775ac49281f61":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c5c08785782b48e08fffeca2be5cda59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9f742f9970945c79493fde95997387c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd48af32a436420ab9970987be0b5c89":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d65645bfa2174875898a8d092b828684":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7fdb2626bc314522bbe75bdcca244511","IPY_MODEL_44b32f2402ad4f98954d2f16c5f43884","IPY_MODEL_2ad0fc2f759844dcaecf25852016abf8"],"layout":"IPY_MODEL_6e2ca4629ac44e239c7307adb21176bb"}},"dcc8a8408c944fed8448d2a347614c05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_12557673ebb6491bba4003a8209fa5c3","max":6286,"min":0,"orientation":"horizontal","style":"IPY_MODEL_deb41f5d630345f2be65569175b7bda2","value":6286}},"dd61074f6ab84781b11e41ff9c91a88c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddcea33b4b4045a5b9ee7fd6a45ec2e7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"de224dfbc0bf405ea0c40d10186214e2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"deb41f5d630345f2be65569175b7bda2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec110c936bb245f9833f5fc7855d7e5f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0a19e438c7940bd99c360270e4f1ff7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2157328d29043ccbd7d663928b9eedf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2e912153e3744dbaf34ca07cad96eb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f428a890581d4729b985f08af336461c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5e24ece9d15484aa7c6004b98a0e853","placeholder":"​","style":"IPY_MODEL_2dff246368ab486b8a0ff0e6b39ca133","value":" 95%"}},"f59ceea710674b4e9a98f2c68c162ef8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"facf646bb731412aa91c9147832990e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faed5a3ed9fb44dcb1d4996371edd23c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff6ce99eacab4bd7b8e63312cc3ae141","IPY_MODEL_8ebf0cd4f6a54fb0a904c7a85084b82b","IPY_MODEL_16bd97625c364d36a3d56c6c7ad00fc8"],"layout":"IPY_MODEL_b30c3a52219646728a630e4beadd1ff4"}},"fc5b0df0a56c4a028e49ac6e7c597234":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00bc13d4c8cb42aab73e5e7814ef067d","IPY_MODEL_3f46cba507164b7a9301d09e9ee9d346","IPY_MODEL_8ac7f9cc06d14c6c8040d0c23bf28daa"],"layout":"IPY_MODEL_3ed954320a5040c3802f07deb361c63a"}},"ff6ce99eacab4bd7b8e63312cc3ae141":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_674ba17ba284486284728ef077acd855","placeholder":"​","style":"IPY_MODEL_b26fd7d0905b450fa5543ce7757495d2","value":"100%"}}}}},"nbformat":4,"nbformat_minor":0}
